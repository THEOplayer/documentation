"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([["79227"],{393998:function(e,t,n){n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>l});var i=n(266751),a=n(785893),s=n(250065);let o={title:"Audio Multiplexing",slug:"/playback/audio-multiplexing",sidebar_position:5},r=void 0,d={},l=[{value:"Understanding Audio Multiplexing",id:"understanding-audio-multiplexing",level:2},{value:"Using Audio Multiplexing",id:"using-audio-multiplexing",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Are audio streams encrypted",id:"are-audio-streams-encrypted",level:3},{value:"Only one audio track is loading",id:"only-one-audio-track-is-loading",level:3},{value:"Audio is dropping in and out",id:"audio-is-dropping-in-and-out",level:3}];function c(e){let t={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"The Dolby.io platform supports Audio Multiplexing, a feature that allows viewers to receive multiple audio streams in a conference-like experience, where each audio stream is emphasized or deemphasized based on activity."}),"\n",(0,a.jsx)(t.h2,{id:"understanding-audio-multiplexing",children:"Understanding Audio Multiplexing"}),"\n",(0,a.jsxs)(t.p,{children:["To first understand Audio Multiplexing, we need to understand how the Dolby.io servers ingest feeds. When ",(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/broadcast",children:"Broadcasting"}),', the Dolby.io Streaming servers will assign the most recent published stream source as the "',(0,a.jsx)(t.em,{children:"Main Source"}),'", a function that allows broadcasters to seamlessly overwrite streams or add ',(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/broadcast/redundant-ingest/",children:"Redundant Streams"}),". For basic broadcasts, this is sufficient, for more advanced broadcasts Dolby.io provides two features that are exceptions:"]}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["Creating a ",(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/broadcast/multi-source-broadcasting",children:"Multisource stream"})," for ",(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/playback/multiview",children:"Multi-view"})," experiences."]}),"\n",(0,a.jsxs)(t.li,{children:['Audio Multiplexing, for experiences with multiple audio streams playing at once, such as that seen on "',(0,a.jsx)(t.em,{children:"Clubhouse"}),'" type platforms and apps.']}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:["If you've reviewed the ",(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/broadcast/multi-source-broadcasting",children:"Multi-source Broadcasting"})," and ",(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/playback/multiview",children:"Multi-view"})," documentation, you may be confused as to why you would use Audio Multiplexing instead of Multiview. Audio multiplexing allows audio sources to overlap, a feature that is useful for"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsx)(t.tr,{children:(0,a.jsxs)(t.th,{style:{textAlign:"left"},children:["Audio Multiplexing",(0,a.jsx)("br",{}),(0,a.jsx)(t.em,{children:"Let users hear multiple audio sources at once that overlap."})]})})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsx)(t.tr,{children:(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"Clubhouse-like apps where hosts chat virtually"})}),(0,a.jsx)(t.tr,{children:(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"Townhall apps or platforms where people may periodically ask questions or present."})}),(0,a.jsx)(t.tr,{children:(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"Immersive sports apps where a commentary track is combined with an in-stadium ambient noise track."})})]})]}),"\n",(0,a.jsx)(t.h2,{id:"using-audio-multiplexing",children:"Using Audio Multiplexing"}),"\n",(0,a.jsxs)(t.p,{children:["To get started using Audio Multiplexing, you first need to create a Publishing token with ",(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/broadcast/multi-source-broadcasting",children:"Multisource"})," and have multiple audio sources ready to test, each assigned a unique ",(0,a.jsx)(t.code,{children:"sourceID"})," at the publisher."]}),"\n",(0,a.jsx)(t.admonition,{title:"Not familar with our JavaScript SDK?",type:"info",children:(0,a.jsxs)(t.p,{children:["Audio Multiplexing is a compelx feature made availible through our ",(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/playback/players-sdks/",children:"Client SDKs"}),"."]})}),"\n",(0,a.jsxs)(t.p,{children:["Once you're streaming multiple audio sources, the next step is to set up the ",(0,a.jsx)(t.a,{href:"/documentation/pr-preview/pr-405/millicast/playback/players-sdks/web/sdk/#viewing-a-stream",children:"Viewer"})," so that the incoming audio sources can be correctly multiplexed. When connecting to the Viewer, there are a ",(0,a.jsx)(t.a,{href:"https://millicast.github.io/millicast-sdk/View.html#connect",children:"number of parameters available in the SDK"})," you can adjust depending on your workflow. Some parameters of note for audio multiplexing include:"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"multiplexedAudioTracks"}),": This is required to enable multiplexing. It denotes the number (",(0,a.jsx)(t.code,{children:"int"}),") of audio tracks to receive as Voice Activity Detection (VAD) multiplexed audio. This value must be greater than or equal to the number of audio tracks on the stream. Additional audio tracks will overwrite existing audio tracks. There isn't a limit to the number of audio tracks that can be rendered in the browser, only the amount of data. The current limit is a bitrate of 12 Mbps."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"dtx"}),": Discontinousous transmision or DTX is a ",(0,a.jsx)(t.code,{children:"boolean"})," value that signals to the viewer to only deliver audio when audio is detected such as when a person is talking. Enabling DTX will reduce bandwidth costs for audio transmission but may cause non-voice audio such as instruments to become choppy."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"pinnedSourceId"}),": Pinned Source ID is a ",(0,a.jsx)(t.code,{children:"String"})," that denotes the main source that will be received by the default MediaStream. This value is useful for denoting your default audio stream that will always load regardless of how many other audio channels are present."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"excludedSourceIds"}),": Exclude Source IDs is an ",(0,a.jsx)(t.code,{children:"Array"})," of ",(0,a.jsx)(t.code,{children:"Strings"})," that denotes audio streams that should not be included in the multiplex. This feature is useful for conference-type applications where a user's ",(0,a.jsx)(t.strong,{children:"own"})," audio shouldn't be heard."]}),"\n"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-javascript",children:'const tokenGenerator = () => millicast.Director.getSubscriber({\n  streamName: "YOUR STREAM NAME",\n  streamAccountId: "YOUR ACCOUNT ID\n});\n\nviewer = new millicast.View("YOUR STREAM NAME", tokenGenerator)\n\nawait viewer.connect({\n  pinnedSourceId: "main",\n  multiplexedAudioTracks: audioElements.length + 1,\n  excludeSourceIds: ["audience1","audience2"],\n  disableVideo: true,\n  dtx: true,\n});\n'})}),"\n",(0,a.jsxs)(t.p,{children:["With the Viewer defined, we can assign the incoming tracks. This can be done in a variety of ways, but the simplest is to grab both the ",(0,a.jsx)(t.code,{children:"track"})," event as it is created by the Viewer node and the ",(0,a.jsx)(t.code,{children:"broadcastEvent"})," as it is created by the Publisher node. Additionally, we also want to create ",(0,a.jsx)(t.code,{children:"<audio>"})," elements and assign them a unique ID. These ",(0,a.jsx)(t.code,{children:"<audio>"})," tags will eventually be what plays the audio stream."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-javascript",children:"const sourceEvents = [];\nconst trackEvents = [];\nconst audioElements = [];\n\nviewer.on('track', async (event) => {\n  console.log('track', event);\n  trackEvents.push(event);\n});\n\nviewer.on('broadcastEvent', async (event) => {\n  console.log('broadcastEvent', event);\n  if (event.name === 'active') {\n    sourceEvents.push(event);\n\n    const newAudio = document.createElement('audio');\n    newAudio.id = 'audioElement' + audioElements.length.toString();\n    audioTrackDiv.appendChild(newAudio);\n    audioElements.push(newAudio);\n    projectOn(audioElements.length + 1);\n  }\n});\n"})}),"\n",(0,a.jsxs)(t.admonition,{title:'Viewer on "track" events',type:"caution",children:[(0,a.jsxs)(t.p,{children:["The Dolby.io SDKs offer ",(0,a.jsx)(t.code,{children:'.on("track",async (event) =>{})'})," functionality for triggering events as tracks are added. When using Audio Multiplexing this event will trigger a number of times equal to the ",(0,a.jsx)(t.code,{children:"multiplexedAudioTracks"})," value, regardless of if those tracks actually contain data."]}),(0,a.jsxs)(t.p,{children:["This means that if ",(0,a.jsx)(t.code,{children:"multiplexedAudioTracks"})," is set to ",(0,a.jsx)(t.code,{children:"5"})," it will trigger once for the first track and five additional times for each multiplexed audio track, regardless of whether there are only two tracks broadcasting or twenty."]})]}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"broadcastEvent"})," will contain the feeds as they are being published. These feeds need to be linked to Viewer ",(0,a.jsx)(t.code,{children:"tracks"})," to be delivered via a function called ",(0,a.jsx)(t.code,{children:"project"})," which ",(0,a.jsx)(t.em,{children:"projects"})," the media onto the track. This is because different feeds maybe be coming from different sources and hence may connect or disconnect at different intervals. The relationship between a feed and a track is organized this way so that as feeds disconnect they can be swapped or removed without all the streams being interrupted."]}),"\n",(0,a.jsxs)(t.p,{children:["The function below also establishes the relationship between the ",(0,a.jsx)(t.code,{children:"<audio>"})," tag we created above and the ",(0,a.jsx)(t.code,{children:"track"}),". Allowing the ",(0,a.jsx)(t.code,{children:"<audio>"})," tag to be rendered on a page for the listener to hear."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-javascript",children:"async function projectOn(index) {\n  const audioElement = audioElements[index];\n  const stream = new MediaStream();\n  stream.addTrack(trackEvents[index].track);\n  audioElement.srcObject = stream;\n  audioElement.play();\n\n  const sourceEvent = sourceEvents[index];\n  const trackEvent = trackEvents[index];\n  console.log('About to project', sourceEvent, trackEvent);\n  await viewer.project(sourceEvent.data.sourceId, [\n    {\n      media: sourceEvent.data.tracks[0].media,\n      trackId: sourceEvent.data.tracks[0].trackId,\n      mediaId: trackEvent.transceiver.mid,\n    },\n  ]);\n}\n"})}),"\n",(0,a.jsx)(t.p,{children:"To recap the above workflow:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsx)(t.li,{children:"The app authenticates and connects with the Dolby.io Streaming (Millicast) Director."}),"\n",(0,a.jsxs)(t.li,{children:["As the app connects the Director creates a main track plus an additional number of tracks equal to the ",(0,a.jsx)(t.code,{children:"multiplexedAudioTracks"})," value. This triggers a ",(0,a.jsx)(t.code,{children:"track"})," event for each track added."]}),"\n",(0,a.jsx)(t.li,{children:"An audio feed is connected to the Publisher Node triggering a Broadcast event."}),"\n",(0,a.jsxs)(t.li,{children:["The broadcast event creates an ",(0,a.jsx)(t.code,{children:"<audio>"})," tag and triggers the ",(0,a.jsx)(t.code,{children:"projectOn"})," function."]}),"\n",(0,a.jsxs)(t.li,{children:["The ",(0,a.jsx)(t.code,{children:"projectOn"})," function adds a track to the ",(0,a.jsx)(t.code,{children:"<audio>"})," tag and then calls the ",(0,a.jsx)(t.code,{children:"viewer.project"})," function to project the media onto the track. Allowing the audio feed to be rendered in the ",(0,a.jsx)(t.code,{children:"<audio>"})," tag."]}),"\n",(0,a.jsx)(t.li,{children:"Steps 3-5 are repeated as more feeds are added."}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"To help with understanding and implementing the Audio Multiplexing feature is included:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-html",children:'<html>\n	<head>\n		<title>Dolby.io Audio Multiplexing Demo</title>\n		<link rel="shortcut icon" href="https://go.dolby.io/hubfs/Dolby_April2021/images/favicon-32x32.png" />\n        \x3c!-- Dolby.io Streaming Millicast JavaScript SDK --\x3e\n		<script src="https://cdn.jsdelivr.net/npm/@millicast/sdk/dist/millicast.umd.min.js"><\/script>\n		\x3c!-- Bootstrap Bundle --\x3e\n		<link\n			href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"\n			rel="stylesheet"\n			integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3"\n			crossorigin="anonymous"\n		/>\n		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css" />\n	</head>\n	<body>\n		<div class="container bg-dark text-white gx-4 px-4 py-4 mt-3 rounded">\n			<img src="https://go.dolby.io/hubfs/raw_assets/public/Dolby_April2021/images/dolby-io-logo.svg" />\n			<h1>Dolby.io Audio Multiplexing Demo</h1>\n		</div>\n		<div class="container px-4 mt-4">\n			<div class="row justify-content-around mt-3">\n				<div class="col-8 shadow p-3 mb-5 bg-body rounded text-center">\n					<button onclick="startStream()" id="startBtn" style={{height: "50px", width: "150px"}}>Start</button>\n					<button onclick="resetStream()" id="resetBtn" style={{height: "50px", width: "150px"}} disabled>Reset</button>\n					,\n					,\n					<div id="audioTrackDiv" color="grey"></div>\n				</div>\n			</div>\n		</div>\n\n		<script>\n			// Your stream Credentials\n			const streamName = "YOUR STREAM NAME";\n			const streamAccountId = "YOUR ACCOUNT ID";\n\n			var viewer;\n			const audioElements = [];\n			const sourceEvents = [];\n			const trackEvents = [];\n			const audioTrackDiv = document.getElementById("audioTrackDiv");\n			const startBtn = document.getElementById("startBtn");\n			const resetBtn = document.getElementById("resetBtn");\n\n			async function startStream() {\n				startBtn.disabled = true;\n				resetBtn.disabled = false;\n				const tokenGenerator = () =>\n					millicast.Director.getSubscriber({\n						streamName: streamName,\n						streamAccountId: streamAccountId,\n					});\n\n				viewer = new millicast.View(streamName, tokenGenerator);\n\n				viewer.on("broadcastEvent", async (event) => {\n					console.log("broadcastEvent", event);\n\n					if (event.name === "active") {\n						const newAudio = document.createElement("audio");\n						newAudio.id = "audioElement" + audioElements.length.toString();\n						newAudio.controls = true;\n						newAudio.autoplay = true;\n						audioTrackDiv.appendChild(newAudio);\n						audioTrackDiv.appendChild(document.createElement("br"));\n						audioTrackDiv.appendChild(document.createElement("br"));\n						audioElements.push(newAudio);\n						sourceEvents.push(event);\n					}\n				});\n\n				viewer.on("track", async (event) => {\n					console.log("track", event);\n					trackEvents.push(event);\n\n					const midInt = parseInt(event.transceiver.mid);\n\n					if (audioElements.length > midInt) {\n						const audioElement = audioElements[midInt];\n\n						const stream = new MediaStream();\n						stream.addTrack(event.track);\n\n						audioElement.srcObject = stream;\n						audioElement.play();\n					}\n				});\n\n				await viewer.connect({\n					pinnedSourceId: "main",\n					multiplexedAudioTracks: audioElements.length + 1,\n					disableVideo: true,\n					dtx: true,\n				});\n				project();\n			}\n\n			async function project() {\n				for (let index = 0; index < sourceEvents.length; index++) {\n					const sourceEvent = sourceEvents[index];\n\n					const trackEvent = trackEvents[index];\n					console.log("About to project", sourceEvent, trackEvent);\n\n					await viewer.project(sourceEvent.data.sourceId, [\n						{\n							media: sourceEvent.data.tracks[0].media,\n							trackId: sourceEvent.data.tracks[0].trackId,\n							mediaId: trackEvent.transceiver.mid,\n						},\n					]);\n				}\n			}\n			function resetStream() {\n				location.reload();\n			}\n		<\/script>\n	</body>\n</html>\n'})}),"\n",(0,a.jsx)(t.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(t.h3,{id:"are-audio-streams-encrypted",children:"Are audio streams encrypted"}),"\n",(0,a.jsx)(t.p,{children:"One of the advantages of using Multiplexing versus audio mixing is the end-to-end encryption as the audio never needs to be processed or re-encoded."}),"\n",(0,a.jsx)(t.h3,{id:"only-one-audio-track-is-loading",children:"Only one audio track is loading"}),"\n",(0,a.jsx)(t.p,{children:"There are a number of reasons why one audio track may only be loading:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Check that the Publishing token is Multisource enabled."}),"\n",(0,a.jsx)(t.li,{children:"Check that each feed has a different source ID."}),"\n",(0,a.jsx)(t.li,{children:"Check that the media stream is being projected onto the correct track."}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"audio-is-dropping-in-and-out",children:"Audio is dropping in and out"}),"\n",(0,a.jsx)(t.p,{children:"There are a number of reasons why audio may be choppy:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"dtx"})," may be interfering with what audio is rendered. It is designed for voice so background noise or instruments may be cut out."]}),"\n",(0,a.jsx)(t.li,{children:"If the sum total bitrate of all audio tracks exceeds 12 Mbps audio may become choppy or drop out."}),"\n"]})]})}function u(e={}){let{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},250065:function(e,t,n){n.d(t,{Z:()=>r,a:()=>o});var i=n(667294);let a={},s=i.createContext(a);function o(e){let t=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:t},e.children)}},266751:function(e){e.exports=JSON.parse('{"id":"playback/audio-multiplexing","title":"Audio Multiplexing","description":"The Dolby.io platform supports Audio Multiplexing, a feature that allows viewers to receive multiple audio streams in a conference-like experience, where each audio stream is emphasized or deemphasized based on activity.","source":"@site/millicast/playback/audio-multiplexing.md","sourceDirName":"playback","slug":"/playback/audio-multiplexing","permalink":"/documentation/pr-preview/pr-405/millicast/playback/audio-multiplexing","draft":false,"unlisted":false,"editUrl":"https://github.com/THEOplayer/documentation/blob/-/millicast/playback/audio-multiplexing.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Audio Multiplexing","slug":"/playback/audio-multiplexing","sidebar_position":5},"sidebar":"millicast","previous":{"title":"Multi-view","permalink":"/documentation/pr-preview/pr-405/millicast/playback/multiview"},"next":{"title":"Frame Metadata","permalink":"/documentation/pr-preview/pr-405/millicast/playback/frame-metadata"}}')}}]);