"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([["57250"],{80687:function(e,i,t){t.d(i,{ZP:function(){return c},d$:function(){return a}});var r=t(85893),n=t(50065);let a=[];function s(e){let i={a:"a",admonition:"admonition",p:"p",...(0,n.a)(),...e.components};return(0,r.jsx)(i.admonition,{title:"Getting Started with iOS SDK",type:"tip",children:(0,r.jsxs)(i.p,{children:["If you haven't already, begin by following the ",(0,r.jsx)(i.a,{href:"/documentation/pr-preview/pr-270/millicast/ios-getting-started-with-subscribing",children:"Getting Started"})," tutorial to become familiar with the concepts to create an application that can publish and/or subscribe using the ",(0,r.jsx)(i.a,{href:"/documentation/pr-preview/pr-270/millicast/ios",children:"iOS"})," SDK."]})})}function c(e={}){let{wrapper:i}={...(0,n.a)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(s,{...e})}):s(e)}},45005:function(e,i,t){t.r(i),t.d(i,{default:()=>h,frontMatter:()=>c,metadata:()=>r,assets:()=>d,toc:()=>l,contentTitle:()=>o});var r=JSON.parse('{"id":"client-sdks/ios/ios-sdk-migration-guide","title":"Migration Guide for iOS SDK","description":"This migration guide provides some tips for upgrading or downgrading between releases of the iOS SDK.","source":"@site/millicast/client-sdks/ios/ios-sdk-migration-guide.mdx","sourceDirName":"client-sdks/ios","slug":"/ios-sdk-migration-guide","permalink":"/documentation/pr-preview/pr-270/millicast/ios-sdk-migration-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/THEOplayer/documentation/blob/-/millicast/client-sdks/ios/ios-sdk-migration-guide.mdx","tags":[],"version":"current","frontMatter":{"title":"Migration Guide for iOS SDK","slug":"/ios-sdk-migration-guide"},"sidebar":"millicast","previous":{"title":"How-to Add Picture in Picture","permalink":"/documentation/pr-preview/pr-270/millicast/ios-how-to-add-picture-in-picture"},"next":{"title":"React Native","permalink":"/documentation/pr-preview/pr-270/millicast/rn"}}'),n=t("85893"),a=t("50065"),s=t("80687");let c={title:"Migration Guide for iOS SDK",slug:"/ios-sdk-migration-guide"},o=void 0,d={},l=[...s.d$,{value:"1.8.x to 2.x",id:"18x-to-2x",level:2},{value:"Subscribing Single Source Streams",id:"subscribing-single-source-streams",level:2},{value:"Subscribing Multi-View Streams",id:"subscribing-multi-view-streams",level:2},{value:"Some key differences in track management",id:"some-key-differences-in-track-management",level:3},{value:"Initialising Video Views",id:"initialising-video-views",level:2},{value:"Querying Video Size and Notifying Video Size changes",id:"querying-video-size-and-notifying-video-size-changes",level:2},{value:"Changelog Overview",id:"changelog-overview",level:2}];function u(e){let i={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,a.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.p,{children:"This migration guide provides some tips for upgrading or downgrading between releases of the iOS SDK."}),"\n","\n",(0,n.jsx)(s.ZP,{}),"\n",(0,n.jsx)(i.h2,{id:"18x-to-2x",children:"1.8.x to 2.x"}),"\n",(0,n.jsxs)(i.p,{children:["Below you'll find examples for migrating your applications from ",(0,n.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v1.8.9",children:"v1.8.9"})," to ",(0,n.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v2.0.0",children:"v2.0.0"})," of the ",(0,n.jsx)(i.a,{href:"/documentation/pr-preview/pr-270/millicast/ios",children:"iOS"})," SDK."]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsxs)(i.p,{children:[(0,n.jsx)(i.strong,{children:(0,n.jsx)(i.em,{children:"NOTE:"})})," Publisher classes remain unchanged and do not require any migration."]}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"subscribing-single-source-streams",children:"Subscribing Single Source Streams"}),"\n",(0,n.jsx)(i.p,{children:"For applications that only have a single source available for playback, track management has changed."}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-swift",metastring:'title="1.8.x"',children:'let subscriber = MCSubscriber()\n\nvar receivedVideoTrack: MCVideoTrack?\n\nTask {\n    for await track in subscriber.tracks() {\n        switch track {\n        case .audio(track: let audioTrack, mid: _):\n            audioTrack.enable(true)\n        case .video(track: let videoTrack, mid: _):\n            videoTrack.enable(true)\n            receivedVideoTrack = videoTrack // Use the received video track for rendering on the UI\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nMCVideoSwiftUIView(videoTrack: receivedVideoTrack)\n'})}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-swift",metastring:'title="2.x"',children:'let subscriber = MCSubscriber()\nlet renderer = MCAcceleratedVideoRenderer()\n\nTask {\n    for await track in subscriber.rtsRemoteTrackAdded() {\n        if let videoTrack = track.asVideo() {\n            try await videoTrack.enable(renderer: renderer) // Use the renderer for displaying video views on the UI\n        } else if let audioTrack = track.asAudio() {\n            try await audioTrack.enable()\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nMCVideoSwiftUIView(renderer: renderer)\n'})}),"\n",(0,n.jsx)(i.h2,{id:"subscribing-multi-view-streams",children:"Subscribing Multi-View Streams"}),"\n",(0,n.jsx)(i.p,{children:"There were a lot of complicated steps involved in projecting the sources for a multi-stream view. This complex workflow requires a lot of understanding of the WebRTC remote tracks. Using the new 2.0.0 implementation removes the need to handle sourceIds, mids and the projection data directly. This solves a lot of complexities from the old implementation."}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-swift",metastring:'title="1.8.x"',children:'let subscriber = MCSubscriber()\n\n// Managing tracks\nvar projectedVideoTracks: [String: MCVideoTrack] = [:]\nvar projectedAudioTracks: [String: MCAudioTrack] = [:]\nvar activeProjectionRequestsForVideo: [String] = []\nvar activeProjectionRequestsForAudio: [String] = []\n\nTask {\n    for await activity in subscriber.activity() {\n        switch activity {\n        case .active(let streamId, let tracks, let sourceId):\n            // Check if the track is already projected\n            // if not, we add a remote track to accept it\n            if tracks.contains("video"), projectedVideoTracks[sourceId] == nil {\n                // This will add the track which will later be received by the task below\n                activeProjectionRequestsForVideo.append(sourceId)\n                await subscriber.addRemoteTrack("video");\n                return\n            } else if tracks.contains("audio"), projectedAudioTracks[sourceId] == nil {\n                activeProjectionRequestsForAudio.append(sourceId)\n                await subscriber.addRemoteTrack("audio");\n                return\n            }\n\n            // if video or audio track already exists, just reproject the track\n            var projectionRequests : [MCProjectionData] = []\n            if projectedVideoTracks[sourceId] != nil {\n                projectionRequests.append(createProjectionInfoWithTrack(projectedVideoTracks[sourceId]))\n            }\n            if projectedAudioTracks[sourceId] != nil, shouldProjectAudioTrack(for: sourceId) {\n                projectionRequests.append(createProjectionInfoWithTrack(projectedAudioTracks[sourceId]))\n            }\n            if (projectionRequests.count > 0) {\n                await subscriber.project(sourceId, projectionRequests)\n            }\n        case .inactive(let streamId, let sourceId):\n            await subscriber.unproject([projectedVideoTracks[sourceId]!.mid, projectedAudioTracks[sourceId]!.mid])\n        }\n    }\n}\n\nTask {\n    for await track in subscriber.tracks() {\n        switch track {\n        case .video(let videoTrack, _):\n          videoTrack.enable(true)\n          // We actually have to somehow know which source ID was requested\n          // to be projected. Since we don\'t know, we have to cache that information\n          // since the SDK returns the track here rather than when calling addRemoteTrack\n          if let sourceId = activeProjectionRequestsForVideo.popFirst() {\n            await subscriber.project(sourceId, createProjectionInfoWithTrack(videoTrack))\n            projectedVideoTracks[sourceId] = videoTrack\n          }\n          // Use `projectedVideoTracks` for display in SwiftUIView\n\n        case .audio(let audioTrack, _):\n          audioTrack.enable(true)\n          if let sourceId = activeProjectionRequestsForAudio.popFirst(), shouldProjectAudioTrack(for: sourceId) {\n            await subscriber.project(sourceId, createProjectionInfoWithTrack(audioTrack))\n            projectedAudioTracks[sourceId] = audioTrack\n          }\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nForEach(projectedVideoTracks.values) { videoTrack in\n    MCVideoSwiftUIView(videoTrack: videoTrack)\n}\n'})}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-swift",metastring:'title="2.x"',children:'var videoRenderers: [String: MCCMSampleBufferVideoRenderer] = [:]\n\nTask {\n    for await track in subscriber.rtsRemoteTrackAdded() {\n        if let videoTrack = track.asVideo() {\n            let renderer = MCCMSampleBufferVideoRenderer()\n            try await videoTrack.enable(renderer: renderer) // Use the renderer for displaying video views on the UI\n            videoRenderers[videoTrack.sourceId] = renderer\n            listenActivityEvent(for: videoTrack, renderer: renderer)\n        } else if let audioTrack = track.asAudio(), shouldProjectAudioTrack(for: audioTrack.sourceId) {\n            try await audioTrack.enable()\n            listenActivityEvent(for: audioTrack)\n        }\n    }\n}\n\nfunc listenActivityEvent(for track: MCRTSRemoteVideoTrack, renderer: MCCMSampleBufferVideoRenderer) {\n    Task {\n        for await activity in track.activity() {\n            switch activity {\n            case .inactive:\n                try await track.disable()\n                videoRenderers[videoTrack.sourceId] = nil\n\n            case .active:\n                try await track.enable(renderer: renderer)\n                videoRenderers[videoTrack.sourceId] = renderer\n            }\n        }\n    }\n}\n\nfunc listenActivityEvent(for track: MCRTSRemoteAudioTrack) {\n    Task {\n        for await activity in track.activity() {\n            switch activity {\n            case .inactive:\n                try await track.disable()\n            case .active:\n                try await track.enable()\n            }\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nForEach(videoRenderers.values) { renderer in\n    MCVideoSwiftUIView(renderer: renderer)\n}\n'})}),"\n",(0,n.jsx)(i.h3,{id:"some-key-differences-in-track-management",children:"Some key differences in track management"}),"\n",(0,n.jsx)(i.p,{children:"1.8.x 2.x Track event emitted is of type TrackEvent which has two types .audio(track: MCAudioTrack, mid: String) and .video(track: MCVideoTrack, mid: String) Track event emits MCRTTSRemoteTrack. This new track type can be a video or audio track, using asVideo() you can verify if it's a video track. Use asAudio() to check if the track is an audio track. Enabling(projecting) tracks is a multistep process - Subscribe to activity() async stream to receive active and inactive sourceId's - Request tracks by calling the addRemoteTrack(:) function - Receive tracks using tracks() async stream - Project mid's using the project(:) API by passing in the MCProjectionData Enabling tracks is now easy - Receive tracks using the rtsRemoteTrackAdded() async stream API - Enable track by calling the enable() for an audio track or enable(renderer:) API for a video track Disable(unproject) tracks using the unproject(:) function Disable tracks using the disable() function layers() async stream is part of MCSubscriber layers() async stream is now part of MCRTSRemoteTrack To select a layer create a new MCProjectionData, set the appropriate layerData and then call project(:) To select a layer call the enable(:) function and pass the MCRTSRemoteVideoTrackLayer to select Activity async stream was part of the MCSubscriber. inactive() event for a particular sourceId indicates the tracks associated with that sourceId are now inactive and active(:) event represents the tracks associated with the sourceId in the event is now active Activity async stream is now part of the MCRTSRemoteTrack. There are two possible values .active and .inactive No APIs to query video and audio tracks You can query video and audio tracks using subscriber.videoTracks() and subscriber.audioTracks()"}),"\n",(0,n.jsx)(i.h2,{id:"initialising-video-views",children:"Initialising Video Views"}),"\n",(0,n.jsx)(i.p,{children:"Initialising a video view for display on the UI has also changed:"}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-swift",metastring:'title="1.8.x"',children:"// Using SwiftUI\nMCVideoSwiftUIView(videoTrack: videoTrack)\n\n// Using UIKit\nlet pipView = MCSampleBufferVideoUIView()\npipView.scalingMode = .aspectFit\npipView.attach(videoTrack: videoTrack, mirrored: false)\n\n// Note: Where `videoTrack` is the instance of MCVideoTrack received\n"})}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-swift",metastring:'title="2.x"',children:"// Using SwiftUI\nMCVideoSwiftUIView(renderer: renderer)\n\n// Using UIKit\nlet videoView = MCSampleBufferVideoUIView(frame: .zero, renderer: renderer)\n\n// Note: Where `renderer` is the instance of MCCMSampleBufferVideoRenderer or MCAccelaratedVideoRenderer used to enable the video track\n"})}),"\n",(0,n.jsx)(i.h2,{id:"querying-video-size-and-notifying-video-size-changes",children:"Querying Video Size and Notifying Video Size changes"}),"\n",(0,n.jsxs)(i.p,{children:[(0,n.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsamplebuffervideouiview",children:"MCSampleBufferVideoUIView"})," or ",(0,n.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcacceleratedvideouiview",children:"MCAcceleratedVideoUIView"})," has properties to query for ",(0,n.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsamplebuffervideouiview/videosize",children:"videoSize"})," and notifies any changes via ",(0,n.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcvideoviewdelegate",children:"MCVideoViewDelegate"}),"."]}),"\n",(0,n.jsxs)(i.p,{children:["These API provide an easy way of querying the videoSize. For example this can be used to set the ",(0,n.jsx)(i.a,{href:"https://developer.apple.com/documentation/uikit/uiviewcontroller/1621476-preferredcontentsize",children:"preferredContentSize"})," in a Picture In Picture presentation."]}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-swift",metastring:'title="1.8.x"',children:"let renderer = MCIosVideoRenderer()\nvideoTrack.add(renderer)\n\n// Get the video size at any given time using the getWidth() and getHeight() APIs\n// Note: There is no API to receive video size changes.\nlet videoSize = CGSizeMake(width: renderer.getWidth(), height: renderer.getHeight())\n"})}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-swift",metastring:'title="2.x"',children:"let videoSize = videoView.videoSize\n\n// In UIKit: listen to the video size changes by conforming to `MCVideoViewDelegate`\nextension MyView: MCVideoViewDelegate {\n    func didChangeVideoSize(_ size: CGSize) {\n        videoSize = size\n    }\n}\n\n// In SwiftUI: use the [onVideoSizeChange](https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcvideoswiftuiview/onvideosizechange(_:)) view modifier\nMCVideoSwiftUIView(renderer: renderer)\n    .onVideoSizeChange {\n        videoSize = $0\n    }\n"})}),"\n",(0,n.jsx)(i.h2,{id:"changelog-overview",children:"Changelog Overview"}),"\n",(0,n.jsxs)(i.p,{children:["Please refer to the changelog for ",(0,n.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v2.0.0",children:"What's new in 2.0.0 SDK Release"})]})]})}function h(e={}){let{wrapper:i}={...(0,a.a)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(u,{...e})}):u(e)}},50065:function(e,i,t){t.d(i,{Z:function(){return c},a:function(){return s}});var r=t(67294);let n={},a=r.createContext(n);function s(e){let i=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function c(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),r.createElement(a.Provider,{value:i},e.children)}}}]);