"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([["54234"],{380687:function(e,i,t){t.d(i,{ZP:()=>c,d$:()=>n});var r=t(785893),a=t(250065);let n=[];function s(e){let i={a:"a",admonition:"admonition",p:"p",...(0,a.a)(),...e.components};return(0,r.jsx)(i.admonition,{title:"Getting Started with iOS SDK",type:"tip",children:(0,r.jsxs)(i.p,{children:["If you haven't already, begin by following the ",(0,r.jsx)(i.a,{href:"/documentation/pr-preview/pr-399/millicast/playback/players-sdks/ios/sdk/getting-started-with-subscribing",children:"Getting Started"})," tutorial to become familiar with the concepts to create an application that can publish and/or subscribe using the ",(0,r.jsx)(i.a,{href:"/documentation/pr-preview/pr-399/millicast/playback/players-sdks/ios/sdk/",children:"iOS"})," SDK."]})})}function c(e={}){let{wrapper:i}={...(0,a.a)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(s,{...e})}):s(e)}},319180:function(e,i,t){t.r(i),t.d(i,{assets:()=>u,contentTitle:()=>l,default:()=>m,frontMatter:()=>d,metadata:()=>r,toc:()=>h});var r=t(946312),a=t(785893),n=t(250065),s=t(308193),c=t(292415),o=t(380687);let d={title:"Migration Guide"},l="Migration Guide for iOS SDK",u={},h=[...o.d$,{value:"1.8.x to 2.x",id:"18x-to-2x",level:2},{value:"Subscribing Single Source Streams",id:"subscribing-single-source-streams",level:2},{value:"Subscribing Multi-View Streams",id:"subscribing-multi-view-streams",level:2},{value:"Some key differences in track management",id:"some-key-differences-in-track-management",level:3},{value:"Initialising Video Views",id:"initialising-video-views",level:2},{value:"Querying Video Size and Notifying Video Size changes",id:"querying-video-size-and-notifying-video-size-changes",level:2},{value:"Changelog Overview",id:"changelog-overview",level:2}];function p(e){let i={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,n.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"migration-guide-for-ios-sdk",children:"Migration Guide for iOS SDK"})}),"\n",(0,a.jsx)(i.p,{children:"This migration guide provides some tips for upgrading or downgrading between releases of the iOS SDK."}),"\n","\n",(0,a.jsx)(o.ZP,{}),"\n",(0,a.jsx)(i.h2,{id:"18x-to-2x",children:"1.8.x to 2.x"}),"\n",(0,a.jsxs)(i.p,{children:["Below you'll find examples for migrating your applications from ",(0,a.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v1.8.9",children:"v1.8.9"})," to ",(0,a.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v2.0.0",children:"v2.0.0"})," of the ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-399/millicast/playback/players-sdks/ios/sdk/",children:"iOS"})," SDK."]}),"\n",(0,a.jsxs)(i.blockquote,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:(0,a.jsx)(i.em,{children:"NOTE:"})})," Publisher classes remain unchanged and do not require any migration."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"subscribing-single-source-streams",children:"Subscribing Single Source Streams"}),"\n",(0,a.jsx)(i.p,{children:"For applications that only have a single source available for playback, track management has changed."}),"\n",(0,a.jsxs)(s.Z,{children:[(0,a.jsx)(c.default,{value:"1.8.x",label:"1.8.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:'let subscriber = MCSubscriber()\n\nvar receivedVideoTrack: MCVideoTrack?\n\nTask {\n    for await track in subscriber.tracks() {\n        switch track {\n        case .audio(track: let audioTrack, mid: _):\n            audioTrack.enable(true)\n        case .video(track: let videoTrack, mid: _):\n            videoTrack.enable(true)\n            receivedVideoTrack = videoTrack // Use the received video track for rendering on the UI\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nMCVideoSwiftUIView(videoTrack: receivedVideoTrack)\n'})})}),(0,a.jsx)(c.default,{value:"2.x",label:"2.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:'let subscriber = MCSubscriber()\nlet renderer = MCAcceleratedVideoRenderer()\n\nTask {\n    for await track in subscriber.rtsRemoteTrackAdded() {\n        if let videoTrack = track.asVideo() {\n            try await videoTrack.enable(renderer: renderer) // Use the renderer for displaying video views on the UI\n        } else if let audioTrack = track.asAudio() {\n            try await audioTrack.enable()\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nMCVideoSwiftUIView(renderer: renderer)\n'})})})]}),"\n",(0,a.jsx)(i.h2,{id:"subscribing-multi-view-streams",children:"Subscribing Multi-View Streams"}),"\n",(0,a.jsx)(i.p,{children:"There were a lot of complicated steps involved in projecting the sources for a multi-stream view. This complex workflow requires a lot of understanding of the WebRTC remote tracks. Using the new 2.0.0 implementation removes the need to handle sourceIds, mids and the projection data directly. This solves a lot of complexities from the old implementation."}),"\n",(0,a.jsxs)(s.Z,{children:[(0,a.jsx)(c.default,{value:"1.8.x",label:"1.8.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:'let subscriber = MCSubscriber()\n\n// Managing tracks\nvar projectedVideoTracks: [String: MCVideoTrack] = [:]\nvar projectedAudioTracks: [String: MCAudioTrack] = [:]\nvar activeProjectionRequestsForVideo: [String] = []\nvar activeProjectionRequestsForAudio: [String] = []\n\nTask {\n    for await activity in subscriber.activity() {\n        switch activity {\n        case .active(let streamId, let tracks, let sourceId):\n            // Check if the track is already projected\n            // if not, we add a remote track to accept it\n            if tracks.contains("video"), projectedVideoTracks[sourceId] == nil {\n                // This will add the track which will later be received by the task below\n                activeProjectionRequestsForVideo.append(sourceId)\n                await subscriber.addRemoteTrack("video");\n                return\n            } else if tracks.contains("audio"), projectedAudioTracks[sourceId] == nil {\n                activeProjectionRequestsForAudio.append(sourceId)\n                await subscriber.addRemoteTrack("audio");\n                return\n            }\n\n            // if video or audio track already exists, just reproject the track\n            var projectionRequests : [MCProjectionData] = []\n            if projectedVideoTracks[sourceId] != nil {\n                projectionRequests.append(createProjectionInfoWithTrack(projectedVideoTracks[sourceId]))\n            }\n            if projectedAudioTracks[sourceId] != nil, shouldProjectAudioTrack(for: sourceId) {\n                projectionRequests.append(createProjectionInfoWithTrack(projectedAudioTracks[sourceId]))\n            }\n            if (projectionRequests.count > 0) {\n                await subscriber.project(sourceId, projectionRequests)\n            }\n        case .inactive(let streamId, let sourceId):\n            await subscriber.unproject([projectedVideoTracks[sourceId]!.mid, projectedAudioTracks[sourceId]!.mid])\n        }\n    }\n}\n\nTask {\n    for await track in subscriber.tracks() {\n        switch track {\n        case .video(let videoTrack, _):\n          videoTrack.enable(true)\n          // We actually have to somehow know which source ID was requested\n          // to be projected. Since we don\'t know, we have to cache that information\n          // since the SDK returns the track here rather than when calling addRemoteTrack\n          if let sourceId = activeProjectionRequestsForVideo.popFirst() {\n            await subscriber.project(sourceId, createProjectionInfoWithTrack(videoTrack))\n            projectedVideoTracks[sourceId] = videoTrack\n          }\n          // Use `projectedVideoTracks` for display in SwiftUIView\n\n        case .audio(let audioTrack, _):\n          audioTrack.enable(true)\n          if let sourceId = activeProjectionRequestsForAudio.popFirst(), shouldProjectAudioTrack(for: sourceId) {\n            await subscriber.project(sourceId, createProjectionInfoWithTrack(audioTrack))\n            projectedAudioTracks[sourceId] = audioTrack\n          }\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nForEach(projectedVideoTracks.values) { videoTrack in\n    MCVideoSwiftUIView(videoTrack: videoTrack)\n}\n'})})}),(0,a.jsx)(c.default,{value:"2.x",label:"2.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:'var videoRenderers: [String: MCCMSampleBufferVideoRenderer] = [:]\n\nTask {\n    for await track in subscriber.rtsRemoteTrackAdded() {\n        if let videoTrack = track.asVideo() {\n            let renderer = MCCMSampleBufferVideoRenderer()\n            try await videoTrack.enable(renderer: renderer) // Use the renderer for displaying video views on the UI\n            videoRenderers[videoTrack.sourceId] = renderer\n            listenActivityEvent(for: videoTrack, renderer: renderer)\n        } else if let audioTrack = track.asAudio(), shouldProjectAudioTrack(for: audioTrack.sourceId) {\n            try await audioTrack.enable()\n            listenActivityEvent(for: audioTrack)\n        }\n    }\n}\n\nfunc listenActivityEvent(for track: MCRTSRemoteVideoTrack, renderer: MCCMSampleBufferVideoRenderer) {\n    Task {\n        for await activity in track.activity() {\n            switch activity {\n            case .inactive:\n                try await track.disable()\n                videoRenderers[videoTrack.sourceId] = nil\n\n            case .active:\n                try await track.enable(renderer: renderer)\n                videoRenderers[videoTrack.sourceId] = renderer\n            }\n        }\n    }\n}\n\nfunc listenActivityEvent(for track: MCRTSRemoteAudioTrack) {\n    Task {\n        for await activity in track.activity() {\n            switch activity {\n            case .inactive:\n                try await track.disable()\n            case .active:\n                try await track.enable()\n            }\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nForEach(videoRenderers.values) { renderer in\n    MCVideoSwiftUIView(renderer: renderer)\n}\n'})})})]}),"\n",(0,a.jsx)(i.h3,{id:"some-key-differences-in-track-management",children:"Some key differences in track management"}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{children:"1.8.x"}),(0,a.jsx)(i.th,{children:"2.x"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:["Track event emitted is of type ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/trackevent",children:"TrackEvent"})," which has two types ",(0,a.jsx)(i.code,{children:".audio(track: MCAudioTrack, mid: String)"})," and ",(0,a.jsx)(i.code,{children:".video(track: MCVideoTrack, mid: String)"})]}),(0,a.jsxs)(i.td,{children:["Track event emits ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack",children:(0,a.jsx)(i.code,{children:"MCRTTSRemoteTrack"})}),". This new track type can be a video or audio track, using ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/asvideo()",children:(0,a.jsx)(i.code,{children:"asVideo()"})})," you can verify if it's a video track. Use ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/asaudio()",children:(0,a.jsx)(i.code,{children:"asAudio()"})})," to check if the track is an audio track."]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:["Enabling(projecting) tracks is a multistep process ",(0,a.jsx)("br",{}),"- Subscribe to ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/activity()",children:"activity()"})," async stream to receive active and inactive sourceId's",(0,a.jsx)("br",{}),"- Request tracks by calling the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/addremotetrack(_:completionhandler:)",children:"addRemoteTrack(:)"})," function",(0,a.jsx)("br",{}),"- Receive tracks using ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/tracks()",children:"tracks()"})," async stream",(0,a.jsx)("br",{}),"- Project mid's using the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/project(_:withdata:completionhandler:)",children:"project(:)"})," API by passing in the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcprojectiondata",children:"MCProjectionData"})]}),(0,a.jsxs)(i.td,{children:["Enabling tracks is now easy",(0,a.jsx)("br",{}),(0,a.jsx)("br",{}),"- Receive tracks using the rtsRemoteTrackAdded() async stream API",(0,a.jsx)("br",{}),"- Enable track by calling the enable() for an audio track or enable(renderer:) API for a video track"]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:["Disable(unproject) tracks using the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/unproject(_:completionhandler:)",children:"unproject(:)"})," function"]}),(0,a.jsxs)(i.td,{children:["Disable tracks using the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/disable(completionhandler:)",children:"disable()"})," function"]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/layers()",children:"layers()"})," async stream is part of MCSubscriber"]}),(0,a.jsxs)(i.td,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/layers()",children:"layers()"})," async stream is now part of MCRTSRemoteTrack"]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:["To select a layer create a new ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcprojectiondata",children:"MCProjectionData"}),", set the appropriate ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcprojectiondata/layer",children:"layerData"})," and then call ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/project(_:withdata:completionhandler:)",children:"project(:)"})]}),(0,a.jsxs)(i.td,{children:["To select a layer call the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotevideotrack/enable(renderer:layer:completionhandler:)",children:"enable(:)"})," function and pass the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotevideotracklayer",children:"MCRTSRemoteVideoTrackLayer"})," to select"]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/activity()",children:"Activity"})," async stream was part of the MCSubscriber. ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/subscriberactivityevent/inactive(streamid:sourceid:)",children:"inactive()"})," event for a particular sourceId indicates the tracks associated with that sourceId are now inactive and ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/subscriberactivityevent/active(streamid:tracks:sourceid:)",children:"active(:)"})," event represents the tracks associated with the ",(0,a.jsx)(i.code,{children:"sourceId"})," in the event is now active"]}),(0,a.jsxs)(i.td,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/activity()",children:"Activity"})," async stream is now part of the MCRTSRemoteTrack. There are two possible values ",(0,a.jsx)(i.code,{children:".active"})," and ",(0,a.jsx)(i.code,{children:".inactive"})]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"No APIs to query video and audio tracks"}),(0,a.jsxs)(i.td,{children:["You can query video and audio tracks using ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsubscriber/videotracks(completionhandler:)",children:"subscriber.videoTracks()"})," and ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsubscriber/audiotracks(completionhandler:)",children:"subscriber.audioTracks()"})]})]})]})]}),"\n",(0,a.jsx)(i.h2,{id:"initialising-video-views",children:"Initialising Video Views"}),"\n",(0,a.jsx)(i.p,{children:"Initialising a video view for display on the UI has also changed:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",metastring:'title="1.8.x"',children:"// Using SwiftUI\nMCVideoSwiftUIView(videoTrack: videoTrack)\n\n// Using UIKit\nlet pipView = MCSampleBufferVideoUIView()\npipView.scalingMode = .aspectFit\npipView.attach(videoTrack: videoTrack, mirrored: false)\n\n// Note: Where `videoTrack` is the instance of MCVideoTrack received\n"})}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",metastring:'title="2.x"',children:"// Using SwiftUI\nMCVideoSwiftUIView(renderer: renderer)\n\n// Using UIKit\nlet videoView = MCSampleBufferVideoUIView(frame: .zero, renderer: renderer)\n\n// Note: Where `renderer` is the instance of MCCMSampleBufferVideoRenderer or MCAccelaratedVideoRenderer used to enable the video track\n"})}),"\n",(0,a.jsx)(i.h2,{id:"querying-video-size-and-notifying-video-size-changes",children:"Querying Video Size and Notifying Video Size changes"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsamplebuffervideouiview",children:"MCSampleBufferVideoUIView"})," or ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcacceleratedvideouiview",children:"MCAcceleratedVideoUIView"})," has properties to query for ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsamplebuffervideouiview/videosize",children:"videoSize"})," and notifies any changes via ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcvideoviewdelegate",children:"MCVideoViewDelegate"}),"."]}),"\n",(0,a.jsxs)(i.p,{children:["These API provide an easy way of querying the videoSize. For example this can be used to set the ",(0,a.jsx)(i.a,{href:"https://developer.apple.com/documentation/uikit/uiviewcontroller/1621476-preferredcontentsize",children:"preferredContentSize"})," in a Picture In Picture presentation."]}),"\n",(0,a.jsxs)(s.Z,{children:[(0,a.jsx)(c.default,{value:"1.8.x",label:"1.8.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:"let renderer = MCIosVideoRenderer()\nvideoTrack.add(renderer)\n\n// Get the video size at any given time using the getWidth() and getHeight() APIs\n// Note: There is no API to receive video size changes.\nlet videoSize = CGSizeMake(width: renderer.getWidth(), height: renderer.getHeight())\n"})})}),(0,a.jsx)(c.default,{value:"2.x",label:"2.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:"let videoSize = videoView.videoSize\n\n// In UIKit: listen to the video size changes by conforming to `MCVideoViewDelegate`\nextension MyView: MCVideoViewDelegate {\n    func didChangeVideoSize(_ size: CGSize) {\n        videoSize = size\n    }\n}\n\n// In SwiftUI: use the [onVideoSizeChange](https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcvideoswiftuiview/onvideosizechange(_:)) view modifier\nMCVideoSwiftUIView(renderer: renderer)\n    .onVideoSizeChange {\n        videoSize = $0\n    }\n"})})})]}),"\n",(0,a.jsx)(i.h2,{id:"changelog-overview",children:"Changelog Overview"}),"\n",(0,a.jsxs)(i.p,{children:["Please refer to the changelog for ",(0,a.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v2.0.0",children:"What's new in 2.0.0 SDK Release"})]})]})}function m(e={}){let{wrapper:i}={...(0,n.a)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},161748:function(e,i,t){t.d(i,{Z:()=>r});let r={tabItem:"tabItem_Ymn6"}},587244:function(e,i,t){t.d(i,{Z:()=>r});let r={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"}},292415:function(e,i,t){t.r(i),t.d(i,{default:()=>s});var r=t(785893);t(667294);var a=t(467026),n=t(161748);function s(e){let{children:i,hidden:t,className:s}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,a.Z)(n.Z.tabItem,s),hidden:t,children:i})}},308193:function(e,i,t){t.d(i,{Z:()=>p});var r=t(785893),a=t(667294),n=t(467026),s=t(69599),c=t(533057),o=t(7227),d=t(587244);function l(e){let{className:i,block:t,selectedValue:a,selectValue:c,tabValues:o}=e,l=[],{blockElementScrollPositionUntilNextRender:u}=(0,s.o5)(),h=e=>{let i=e.currentTarget,t=o[l.indexOf(i)].value;t!==a&&(u(i),c(t))},p=e=>{let i=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":{let t=l.indexOf(e.currentTarget)+1;i=l[t]??l[0];break}case"ArrowLeft":{let t=l.indexOf(e.currentTarget)-1;i=l[t]??l[l.length-1]}}i?.focus()};return(0,r.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,n.Z)("tabs",{"tabs--block":t},i),children:o.map(e=>{let{value:i,label:t,attributes:s}=e;return(0,r.jsx)("li",{role:"tab",tabIndex:a===i?0:-1,"aria-selected":a===i,ref:e=>{l.push(e)},onKeyDown:p,onClick:h,...s,className:(0,n.Z)("tabs__item",d.Z.tabItem,s?.className,{"tabs__item--active":a===i}),children:t??i},i)})})}function u(e){let{lazy:i,children:t,selectedValue:s}=e,c=(Array.isArray(t)?t:[t]).filter(Boolean);if(i){let e=c.find(e=>e.props.value===s);return e?(0,a.cloneElement)(e,{className:(0,n.Z)("margin-top--md",e.props.className)}):null}return(0,r.jsx)("div",{className:"margin-top--md",children:c.map((e,i)=>(0,a.cloneElement)(e,{key:i,hidden:e.props.value!==s}))})}function h(e){let i=(0,c.Y)(e);return(0,r.jsxs)("div",{className:(0,n.Z)("tabs-container",d.Z.tabList),children:[(0,r.jsx)(l,{...i,...e}),(0,r.jsx)(u,{...i,...e})]})}function p(e){let i=(0,o.default)();return(0,r.jsx)(h,{...e,children:(0,c.h)(e.children)},String(i))}},533057:function(e,i,t){t.d(i,{Y:()=>u,h:()=>d});var r=t(667294),a=t(616550),n=t(232e3),s=t(4520),c=t(38341),o=t(768737);function d(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){let{props:i}=e;return!!i&&"object"==typeof i&&"value"in i}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function l(e){let{value:i,tabValues:t}=e;return t.some(e=>e.value===i)}function u(e){let{defaultValue:i,queryString:t=!1,groupId:u}=e,h=function(e){let{values:i,children:t}=e;return(0,r.useMemo)(()=>{let e=i??d(t).map(e=>{let{props:{value:i,label:t,attributes:r,default:a}}=e;return{value:i,label:t,attributes:r,default:a}}),r=(0,c.lx)(e,(e,i)=>e.value===i.value);if(r.length>0)throw Error(`Docusaurus error: Duplicate values "${r.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[i,t])}(e),[p,m]=(0,r.useState)(()=>(function(e){let{defaultValue:i,tabValues:t}=e;if(0===t.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(i){if(!l({value:i,tabValues:t}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${i}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return i}let r=t.find(e=>e.default)??t[0];if(!r)throw Error("Unexpected error: 0 tabValues");return r.value})({defaultValue:i,tabValues:h})),[b,k]=function(e){let{queryString:i=!1,groupId:t}=e,n=(0,a.k6)(),c=function(e){let{queryString:i=!1,groupId:t}=e;if("string"==typeof i)return i;if(!1===i)return null;if(!0===i&&!t)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:i,groupId:t});return[(0,s._X)(c),(0,r.useCallback)(e=>{if(!c)return;let i=new URLSearchParams(n.location.search);i.set(c,e),n.replace({...n.location,search:i.toString()})},[c,n])]}({queryString:t,groupId:u}),[v,f]=function(e){let{groupId:i}=e,t=i?`docusaurus.tab.${i}`:null,[a,n]=(0,o.Nk)(t);return[a,(0,r.useCallback)(e=>{t&&n.set(e)},[t,n])]}({groupId:u}),g=(()=>{let e=b??v;return l({value:e,tabValues:h})?e:null})();return(0,n.Z)(()=>{g&&m(g)},[g]),{selectedValue:p,selectValue:(0,r.useCallback)(e=>{if(!l({value:e,tabValues:h}))throw Error(`Can't select invalid tab value=${e}`);m(e),k(e),f(e)},[k,f,h]),tabValues:h}}},250065:function(e,i,t){t.d(i,{Z:()=>c,a:()=>s});var r=t(667294);let a={},n=r.createContext(a);function s(e){let i=r.useContext(n);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function c(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(n.Provider,{value:i},e.children)}},946312:function(e){e.exports=JSON.parse('{"id":"playback/players-sdks/ios/sdk/sdk-migration-guide","title":"Migration Guide","description":"This migration guide provides some tips for upgrading or downgrading between releases of the iOS SDK.","source":"@site/millicast/playback/players-sdks/ios/sdk/sdk-migration-guide.mdx","sourceDirName":"playback/players-sdks/ios/sdk","slug":"/playback/players-sdks/ios/sdk/sdk-migration-guide","permalink":"/documentation/pr-preview/pr-399/millicast/playback/players-sdks/ios/sdk/sdk-migration-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/THEOplayer/documentation/blob/-/millicast/playback/players-sdks/ios/sdk/sdk-migration-guide.mdx","tags":[],"version":"current","frontMatter":{"title":"Migration Guide"},"sidebar":"millicast","previous":{"title":"How-to Add Picture in Picture","permalink":"/documentation/pr-preview/pr-399/millicast/playback/players-sdks/ios/sdk/how-to-add-picture-in-picture"},"next":{"title":"iOS Viewer","permalink":"/documentation/pr-preview/pr-399/millicast/playback/players-sdks/ios/samples/sample-apps-ios-viewer"}}')}}]);