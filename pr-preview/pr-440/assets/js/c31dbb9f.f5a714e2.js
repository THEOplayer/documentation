"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([["18620"],{591320:function(e,i,t){t.d(i,{Ay:()=>c,RM:()=>n});var r=t(474848),a=t(884429);let n=[];function s(e){let i={a:"a",admonition:"admonition",p:"p",...(0,a.R)(),...e.components};return(0,r.jsx)(i.admonition,{title:"Getting Started with iOS SDK",type:"tip",children:(0,r.jsxs)(i.p,{children:["If you haven't already, begin by following the ",(0,r.jsx)(i.a,{href:"/documentation/pr-preview/pr-440/millicast/playback/players-sdks/ios/sdk/getting-started-with-subscribing",children:"Getting Started"})," tutorial to become familiar with the concepts to create an application that can publish and/or subscribe using the ",(0,r.jsx)(i.a,{href:"/documentation/pr-preview/pr-440/millicast/playback/players-sdks/ios/sdk/",children:"iOS"})," SDK."]})})}function c(e={}){let{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(s,{...e})}):s(e)}},476399:function(e,i,t){t.r(i),t.d(i,{assets:()=>u,contentTitle:()=>l,default:()=>m,frontMatter:()=>d,metadata:()=>r,toc:()=>h});var r=t(867892),a=t(474848),n=t(884429),s=t(733021),c=t(372207),o=t(591320);let d={title:"Migration Guide"},l="Migration Guide for iOS SDK",u={},h=[...o.RM,{value:"1.8.x to 2.x",id:"18x-to-2x",level:2},{value:"Subscribing Single Source Streams",id:"subscribing-single-source-streams",level:2},{value:"Subscribing Multi-View Streams",id:"subscribing-multi-view-streams",level:2},{value:"Some key differences in track management",id:"some-key-differences-in-track-management",level:3},{value:"Initialising Video Views",id:"initialising-video-views",level:2},{value:"Querying Video Size and Notifying Video Size changes",id:"querying-video-size-and-notifying-video-size-changes",level:2},{value:"Changelog Overview",id:"changelog-overview",level:2}];function p(e){let i={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,n.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"migration-guide-for-ios-sdk",children:"Migration Guide for iOS SDK"})}),"\n",(0,a.jsx)(i.p,{children:"This migration guide provides some tips for upgrading or downgrading between releases of the iOS SDK."}),"\n","\n",(0,a.jsx)(o.Ay,{}),"\n",(0,a.jsx)(i.h2,{id:"18x-to-2x",children:"1.8.x to 2.x"}),"\n",(0,a.jsxs)(i.p,{children:["Below you'll find examples for migrating your applications from ",(0,a.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v1.8.9",children:"v1.8.9"})," to ",(0,a.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v2.0.0",children:"v2.0.0"})," of the ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-440/millicast/playback/players-sdks/ios/sdk/",children:"iOS"})," SDK."]}),"\n",(0,a.jsxs)(i.blockquote,{children:["\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:(0,a.jsx)(i.em,{children:"NOTE:"})})," Publisher classes remain unchanged and do not require any migration."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"subscribing-single-source-streams",children:"Subscribing Single Source Streams"}),"\n",(0,a.jsx)(i.p,{children:"For applications that only have a single source available for playback, track management has changed."}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(c.default,{value:"1.8.x",label:"1.8.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:'let subscriber = MCSubscriber()\n\nvar receivedVideoTrack: MCVideoTrack?\n\nTask {\n    for await track in subscriber.tracks() {\n        switch track {\n        case .audio(track: let audioTrack, mid: _):\n            audioTrack.enable(true)\n        case .video(track: let videoTrack, mid: _):\n            videoTrack.enable(true)\n            receivedVideoTrack = videoTrack // Use the received video track for rendering on the UI\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nMCVideoSwiftUIView(videoTrack: receivedVideoTrack)\n'})})}),(0,a.jsx)(c.default,{value:"2.x",label:"2.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:'let subscriber = MCSubscriber()\nlet renderer = MCAcceleratedVideoRenderer()\n\nTask {\n    for await track in subscriber.rtsRemoteTrackAdded() {\n        if let videoTrack = track.asVideo() {\n            try await videoTrack.enable(renderer: renderer) // Use the renderer for displaying video views on the UI\n        } else if let audioTrack = track.asAudio() {\n            try await audioTrack.enable()\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nMCVideoSwiftUIView(renderer: renderer)\n'})})})]}),"\n",(0,a.jsx)(i.h2,{id:"subscribing-multi-view-streams",children:"Subscribing Multi-View Streams"}),"\n",(0,a.jsx)(i.p,{children:"There were a lot of complicated steps involved in projecting the sources for a multi-stream view. This complex workflow requires a lot of understanding of the WebRTC remote tracks. Using the new 2.0.0 implementation removes the need to handle sourceIds, mids and the projection data directly. This solves a lot of complexities from the old implementation."}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(c.default,{value:"1.8.x",label:"1.8.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:'let subscriber = MCSubscriber()\n\n// Managing tracks\nvar projectedVideoTracks: [String: MCVideoTrack] = [:]\nvar projectedAudioTracks: [String: MCAudioTrack] = [:]\nvar activeProjectionRequestsForVideo: [String] = []\nvar activeProjectionRequestsForAudio: [String] = []\n\nTask {\n    for await activity in subscriber.activity() {\n        switch activity {\n        case .active(let streamId, let tracks, let sourceId):\n            // Check if the track is already projected\n            // if not, we add a remote track to accept it\n            if tracks.contains("video"), projectedVideoTracks[sourceId] == nil {\n                // This will add the track which will later be received by the task below\n                activeProjectionRequestsForVideo.append(sourceId)\n                await subscriber.addRemoteTrack("video");\n                return\n            } else if tracks.contains("audio"), projectedAudioTracks[sourceId] == nil {\n                activeProjectionRequestsForAudio.append(sourceId)\n                await subscriber.addRemoteTrack("audio");\n                return\n            }\n\n            // if video or audio track already exists, just reproject the track\n            var projectionRequests : [MCProjectionData] = []\n            if projectedVideoTracks[sourceId] != nil {\n                projectionRequests.append(createProjectionInfoWithTrack(projectedVideoTracks[sourceId]))\n            }\n            if projectedAudioTracks[sourceId] != nil, shouldProjectAudioTrack(for: sourceId) {\n                projectionRequests.append(createProjectionInfoWithTrack(projectedAudioTracks[sourceId]))\n            }\n            if (projectionRequests.count > 0) {\n                await subscriber.project(sourceId, projectionRequests)\n            }\n        case .inactive(let streamId, let sourceId):\n            await subscriber.unproject([projectedVideoTracks[sourceId]!.mid, projectedAudioTracks[sourceId]!.mid])\n        }\n    }\n}\n\nTask {\n    for await track in subscriber.tracks() {\n        switch track {\n        case .video(let videoTrack, _):\n          videoTrack.enable(true)\n          // We actually have to somehow know which source ID was requested\n          // to be projected. Since we don\'t know, we have to cache that information\n          // since the SDK returns the track here rather than when calling addRemoteTrack\n          if let sourceId = activeProjectionRequestsForVideo.popFirst() {\n            await subscriber.project(sourceId, createProjectionInfoWithTrack(videoTrack))\n            projectedVideoTracks[sourceId] = videoTrack\n          }\n          // Use `projectedVideoTracks` for display in SwiftUIView\n\n        case .audio(let audioTrack, _):\n          audioTrack.enable(true)\n          if let sourceId = activeProjectionRequestsForAudio.popFirst(), shouldProjectAudioTrack(for: sourceId) {\n            await subscriber.project(sourceId, createProjectionInfoWithTrack(audioTrack))\n            projectedAudioTracks[sourceId] = audioTrack\n          }\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nForEach(projectedVideoTracks.values) { videoTrack in\n    MCVideoSwiftUIView(videoTrack: videoTrack)\n}\n'})})}),(0,a.jsx)(c.default,{value:"2.x",label:"2.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:'var videoRenderers: [String: MCCMSampleBufferVideoRenderer] = [:]\n\nTask {\n    for await track in subscriber.rtsRemoteTrackAdded() {\n        if let videoTrack = track.asVideo() {\n            let renderer = MCCMSampleBufferVideoRenderer()\n            try await videoTrack.enable(renderer: renderer) // Use the renderer for displaying video views on the UI\n            videoRenderers[videoTrack.sourceId] = renderer\n            listenActivityEvent(for: videoTrack, renderer: renderer)\n        } else if let audioTrack = track.asAudio(), shouldProjectAudioTrack(for: audioTrack.sourceId) {\n            try await audioTrack.enable()\n            listenActivityEvent(for: audioTrack)\n        }\n    }\n}\n\nfunc listenActivityEvent(for track: MCRTSRemoteVideoTrack, renderer: MCCMSampleBufferVideoRenderer) {\n    Task {\n        for await activity in track.activity() {\n            switch activity {\n            case .inactive:\n                try await track.disable()\n                videoRenderers[videoTrack.sourceId] = nil\n\n            case .active:\n                try await track.enable(renderer: renderer)\n                videoRenderers[videoTrack.sourceId] = renderer\n            }\n        }\n    }\n}\n\nfunc listenActivityEvent(for track: MCRTSRemoteAudioTrack) {\n    Task {\n        for await activity in track.activity() {\n            switch activity {\n            case .inactive:\n                try await track.disable()\n            case .active:\n                try await track.enable()\n            }\n        }\n    }\n}\n\nlet credentials = MCSubscriberCredentials()\ncredentials.streamName =  "STREAM_NAME";\ncredentials.accountId = "ACCOUNT_ID";\ncredentials.apiUrl = "https://director.millicast.com/api/director/subscribe";\n\ntry await subscriber.setCredentials(credentials)\ntry await subscriber.connect()\ntry await subscriber.subscribe()\n\n// For displaying on the SwiftUI view\nForEach(videoRenderers.values) { renderer in\n    MCVideoSwiftUIView(renderer: renderer)\n}\n'})})})]}),"\n",(0,a.jsx)(i.h3,{id:"some-key-differences-in-track-management",children:"Some key differences in track management"}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{children:"1.8.x"}),(0,a.jsx)(i.th,{children:"2.x"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:["Track event emitted is of type ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/trackevent",children:"TrackEvent"})," which has two types ",(0,a.jsx)(i.code,{children:".audio(track: MCAudioTrack, mid: String)"})," and ",(0,a.jsx)(i.code,{children:".video(track: MCVideoTrack, mid: String)"})]}),(0,a.jsxs)(i.td,{children:["Track event emits ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack",children:(0,a.jsx)(i.code,{children:"MCRTTSRemoteTrack"})}),". This new track type can be a video or audio track, using ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/asvideo()",children:(0,a.jsx)(i.code,{children:"asVideo()"})})," you can verify if it's a video track. Use ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/asaudio()",children:(0,a.jsx)(i.code,{children:"asAudio()"})})," to check if the track is an audio track."]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:["Enabling(projecting) tracks is a multistep process ",(0,a.jsx)("br",{}),"- Subscribe to ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/activity()",children:"activity()"})," async stream to receive active and inactive sourceId's",(0,a.jsx)("br",{}),"- Request tracks by calling the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/addremotetrack(_:completionhandler:)",children:"addRemoteTrack(:)"})," function",(0,a.jsx)("br",{}),"- Receive tracks using ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/tracks()",children:"tracks()"})," async stream",(0,a.jsx)("br",{}),"- Project mid's using the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/project(_:withdata:completionhandler:)",children:"project(:)"})," API by passing in the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcprojectiondata",children:"MCProjectionData"})]}),(0,a.jsxs)(i.td,{children:["Enabling tracks is now easy",(0,a.jsx)("br",{}),(0,a.jsx)("br",{}),"- Receive tracks using the rtsRemoteTrackAdded() async stream API",(0,a.jsx)("br",{}),"- Enable track by calling the enable() for an audio track or enable(renderer:) API for a video track"]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:["Disable(unproject) tracks using the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/unproject(_:completionhandler:)",children:"unproject(:)"})," function"]}),(0,a.jsxs)(i.td,{children:["Disable tracks using the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/disable(completionhandler:)",children:"disable()"})," function"]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/layers()",children:"layers()"})," async stream is part of MCSubscriber"]}),(0,a.jsxs)(i.td,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/layers()",children:"layers()"})," async stream is now part of MCRTSRemoteTrack"]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:["To select a layer create a new ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcprojectiondata",children:"MCProjectionData"}),", set the appropriate ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcprojectiondata/layer",children:"layerData"})," and then call ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/project(_:withdata:completionhandler:)",children:"project(:)"})]}),(0,a.jsxs)(i.td,{children:["To select a layer call the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotevideotrack/enable(renderer:layer:completionhandler:)",children:"enable(:)"})," function and pass the ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotevideotracklayer",children:"MCRTSRemoteVideoTrackLayer"})," to select"]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/mcsubscriber/activity()",children:"Activity"})," async stream was part of the MCSubscriber. ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/subscriberactivityevent/inactive(streamid:sourceid:)",children:"inactive()"})," event for a particular sourceId indicates the tracks associated with that sourceId are now inactive and ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/1.8.9/apple/documentation/millicastsdk/subscriberactivityevent/active(streamid:tracks:sourceid:)",children:"active(:)"})," event represents the tracks associated with the ",(0,a.jsx)(i.code,{children:"sourceId"})," in the event is now active"]}),(0,a.jsxs)(i.td,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcrtsremotetrack/activity()",children:"Activity"})," async stream is now part of the MCRTSRemoteTrack. There are two possible values ",(0,a.jsx)(i.code,{children:".active"})," and ",(0,a.jsx)(i.code,{children:".inactive"})]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"No APIs to query video and audio tracks"}),(0,a.jsxs)(i.td,{children:["You can query video and audio tracks using ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsubscriber/videotracks(completionhandler:)",children:"subscriber.videoTracks()"})," and ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsubscriber/audiotracks(completionhandler:)",children:"subscriber.audioTracks()"})]})]})]})]}),"\n",(0,a.jsx)(i.h2,{id:"initialising-video-views",children:"Initialising Video Views"}),"\n",(0,a.jsx)(i.p,{children:"Initialising a video view for display on the UI has also changed:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",metastring:'title="1.8.x"',children:"// Using SwiftUI\nMCVideoSwiftUIView(videoTrack: videoTrack)\n\n// Using UIKit\nlet pipView = MCSampleBufferVideoUIView()\npipView.scalingMode = .aspectFit\npipView.attach(videoTrack: videoTrack, mirrored: false)\n\n// Note: Where `videoTrack` is the instance of MCVideoTrack received\n"})}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",metastring:'title="2.x"',children:"// Using SwiftUI\nMCVideoSwiftUIView(renderer: renderer)\n\n// Using UIKit\nlet videoView = MCSampleBufferVideoUIView(frame: .zero, renderer: renderer)\n\n// Note: Where `renderer` is the instance of MCCMSampleBufferVideoRenderer or MCAccelaratedVideoRenderer used to enable the video track\n"})}),"\n",(0,a.jsx)(i.h2,{id:"querying-video-size-and-notifying-video-size-changes",children:"Querying Video Size and Notifying Video Size changes"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsamplebuffervideouiview",children:"MCSampleBufferVideoUIView"})," or ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcacceleratedvideouiview",children:"MCAcceleratedVideoUIView"})," has properties to query for ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcsamplebuffervideouiview/videosize",children:"videoSize"})," and notifies any changes via ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcvideoviewdelegate",children:"MCVideoViewDelegate"}),"."]}),"\n",(0,a.jsxs)(i.p,{children:["These API provide an easy way of querying the videoSize. For example this can be used to set the ",(0,a.jsx)(i.a,{href:"https://developer.apple.com/documentation/uikit/uiviewcontroller/1621476-preferredcontentsize",children:"preferredContentSize"})," in a Picture In Picture presentation."]}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(c.default,{value:"1.8.x",label:"1.8.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:"let renderer = MCIosVideoRenderer()\nvideoTrack.add(renderer)\n\n// Get the video size at any given time using the getWidth() and getHeight() APIs\n// Note: There is no API to receive video size changes.\nlet videoSize = CGSizeMake(width: renderer.getWidth(), height: renderer.getHeight())\n"})})}),(0,a.jsx)(c.default,{value:"2.x",label:"2.x",children:(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-swift",children:"let videoSize = videoView.videoSize\n\n// In UIKit: listen to the video size changes by conforming to `MCVideoViewDelegate`\nextension MyView: MCVideoViewDelegate {\n    func didChangeVideoSize(_ size: CGSize) {\n        videoSize = size\n    }\n}\n\n// In SwiftUI: use the [onVideoSizeChange](https://millicast.github.io/doc/latest/apple/documentation/millicastsdk/mcvideoswiftuiview/onvideosizechange(_:)) view modifier\nMCVideoSwiftUIView(renderer: renderer)\n    .onVideoSizeChange {\n        videoSize = $0\n    }\n"})})})]}),"\n",(0,a.jsx)(i.h2,{id:"changelog-overview",children:"Changelog Overview"}),"\n",(0,a.jsxs)(i.p,{children:["Please refer to the changelog for ",(0,a.jsx)(i.a,{href:"https://github.com/millicast/millicast-native-sdk/releases/tag/v2.0.0",children:"What's new in 2.0.0 SDK Release"})]})]})}function m(e={}){let{wrapper:i}={...(0,n.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},403118:function(e,i,t){t.d(i,{A:()=>r});let r={tabItem:"tabItem_Ymn6"}},827778:function(e,i,t){t.d(i,{A:()=>r});let r={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"}},372207:function(e,i,t){t.r(i),t.d(i,{default:()=>s});var r=t(474848);t(296540);var a=t(39836),n=t(403118);function s({children:e,hidden:i,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,a.A)(n.A.tabItem,t),hidden:i,children:e})}},733021:function(e,i,t){t.d(i,{A:()=>m});var r=t(474848),a=t(296540),n=t(39836),s=t(416364),c=t(568251),o=t(243272),d=t(298864),l=t(827778);function u({className:e,block:i,selectedValue:t,selectValue:a,tabValues:s}){let o=[],{blockElementScrollPositionUntilNextRender:d}=(0,c.a_)(),u=e=>{let i=e.currentTarget,r=s[o.indexOf(i)].value;r!==t&&(d(i),a(r))},h=e=>{let i=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{let t=o.indexOf(e.currentTarget)+1;i=o[t]??o[0];break}case"ArrowLeft":{let t=o.indexOf(e.currentTarget)-1;i=o[t]??o[o.length-1]}}i?.focus()};return(0,r.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,n.A)("tabs",{"tabs--block":i},e),children:s.map(({value:e,label:i,attributes:a})=>(0,r.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:h,onClick:u,...a,className:(0,n.A)("tabs__item",l.A.tabItem,a?.className,{"tabs__item--active":t===e}),children:i??e},e))})}function h({lazy:e,children:i,selectedValue:t}){let s=(Array.isArray(i)?i:[i]).filter(Boolean);if(e){let e=s.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,n.A)("margin-top--md",e.props.className)}):null}return(0,r.jsx)("div",{className:"margin-top--md",children:s.map((e,i)=>(0,a.cloneElement)(e,{key:i,hidden:e.props.value!==t}))})}function p(e){let i=(0,o.u)(e);return(0,r.jsxs)("div",{className:(0,n.A)(s.G.tabs.container,"tabs-container",l.A.tabList),children:[(0,r.jsx)(u,{...i,...e}),(0,r.jsx)(h,{...i,...e})]})}function m(e){let i=(0,d.default)();return(0,r.jsx)(p,{...e,children:(0,o.v)(e.children)},String(i))}},243272:function(e,i,t){t.d(i,{u:()=>u,v:()=>d});var r=t(296540),a=t(956347),n=t(628004),s=t(425580),c=t(712213),o=t(165023);function d(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){let{props:i}=e;return!!i&&"object"==typeof i&&"value"in i}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function l({value:e,tabValues:i}){return i.some(i=>i.value===e)}function u(e){let i,{defaultValue:t,queryString:u=!1,groupId:h}=e,p=function(e){let{values:i,children:t}=e;return(0,r.useMemo)(()=>{let e=i??d(t).map(({props:{value:e,label:i,attributes:t,default:r}})=>({value:e,label:i,attributes:t,default:r})),r=(0,c.XI)(e,(e,i)=>e.value===i.value);if(r.length>0)throw Error(`Docusaurus error: Duplicate values "${r.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[i,t])}(e),[m,b]=(0,r.useState)(()=>(function({defaultValue:e,tabValues:i}){if(0===i.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!l({value:e,tabValues:i}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${i.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let t=i.find(e=>e.default)??i[0];if(!t)throw Error("Unexpected error: 0 tabValues");return t.value})({defaultValue:t,tabValues:p})),[v,k]=function({queryString:e=!1,groupId:i}){let t=(0,a.W6)(),n=function({queryString:e=!1,groupId:i}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!i)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return i??null}({queryString:e,groupId:i});return[(0,s.aZ)(n),(0,r.useCallback)(e=>{if(!n)return;let i=new URLSearchParams(t.location.search);i.set(n,e),t.replace({...t.location,search:i.toString()})},[n,t])]}({queryString:u,groupId:h}),[f,g]=function({groupId:e}){let i=e?`docusaurus.tab.${e}`:null,[t,a]=(0,o.Dv)(i);return[t,(0,r.useCallback)(e=>{i&&a.set(e)},[i,a])]}({groupId:h}),j=l({value:i=v??f,tabValues:p})?i:null;return(0,n.A)(()=>{j&&b(j)},[j]),{selectedValue:m,selectValue:(0,r.useCallback)(e=>{if(!l({value:e,tabValues:p}))throw Error(`Can't select invalid tab value=${e}`);b(e),k(e),g(e)},[k,g,p]),tabValues:p}}},884429:function(e,i,t){t.d(i,{R:()=>s,x:()=>c});var r=t(296540);let a={},n=r.createContext(a);function s(e){let i=r.useContext(n);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function c(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(n.Provider,{value:i},e.children)}},867892:function(e){e.exports=JSON.parse('{"id":"playback/players-sdks/ios/sdk/sdk-migration-guide","title":"Migration Guide","description":"This migration guide provides some tips for upgrading or downgrading between releases of the iOS SDK.","source":"@site/millicast/playback/players-sdks/ios/sdk/sdk-migration-guide.mdx","sourceDirName":"playback/players-sdks/ios/sdk","slug":"/playback/players-sdks/ios/sdk/sdk-migration-guide","permalink":"/documentation/pr-preview/pr-440/millicast/playback/players-sdks/ios/sdk/sdk-migration-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/THEOplayer/documentation/blob/-/millicast/playback/players-sdks/ios/sdk/sdk-migration-guide.mdx","tags":[],"version":"current","frontMatter":{"title":"Migration Guide"},"sidebar":"millicast","previous":{"title":"How-to Add Picture in Picture","permalink":"/documentation/pr-preview/pr-440/millicast/playback/players-sdks/ios/sdk/how-to-add-picture-in-picture"},"next":{"title":"iOS Viewer","permalink":"/documentation/pr-preview/pr-440/millicast/playback/players-sdks/ios/samples/sample-apps-ios-viewer"}}')}}]);