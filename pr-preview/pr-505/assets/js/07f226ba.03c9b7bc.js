"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([["17612"],{669022(e,i,t){t.r(i),t.d(i,{assets:()=>l,contentTitle:()=>d,default:()=>m,frontMatter:()=>o,metadata:()=>n,toc:()=>c});var n=t(585501),a=t(474848),r=t(28453),s=t(778093);let o={title:"Multi-view",slug:"/playback/multiview",sidebar_position:4},d,l={},c=[{value:"Multi-view with the Dolby.io viewer",id:"multi-view-with-the-dolbyio-viewer",level:2},{value:"Creating a Multi-view web application",id:"creating-a-multi-view-web-application",level:2},{value:"Store and track incoming Multisource feeds",id:"store-and-track-incoming-multisource-feeds",level:3},{value:"Add video elements and render feeds",id:"add-video-elements-and-render-feeds",level:3},{value:"Final result",id:"final-result",level:3},{value:"Assigning lower-quality layers to small tiles",id:"assigning-lower-quality-layers-to-small-tiles",level:2},{value:"Limitations of Multi-view",id:"limitations-of-multi-view",level:2}];function h(e){let i={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.p,{children:"Multi-view lets you ingest and render multiple Dolby.io real-time video and audio streams simultaneously inside a browser or mobile native applications. Once rendered, you can switch seamlessly between streams, allowing you to control how you view the content. By giving viewers content control, broadcasters can enable real-time experiences and engagement that leave viewers wanting more."}),"\n",(0,a.jsx)("div",{className:"youtube-container",children:(0,a.jsx)("iframe",{width:"560",height:"315",src:"https://viewer.millicast.com/?streamId=k9Mwad/multiview&multisource=true&mute=true"})}),"\n",(0,a.jsxs)(i.p,{children:["To create a multi-view experience you must capture multiple video or audio feeds and then broadcast them as a ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/broadcast/multi-source-broadcasting",children:"multi-source stream"}),". Once broadcasting a multi-source stream, you can view the stream using the Dolby.io Millicast viewer app, or by building your own multi-view application. Dolby.io also supports ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/audio-multiplexing",children:"Audio Multiplexing"})," for mixed audio playback."]}),"\n",(0,a.jsx)(i.h2,{id:"multi-view-with-the-dolbyio-viewer",children:"Multi-view with the Dolby.io viewer"}),"\n",(0,a.jsxs)(i.p,{children:["Once you have created a ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/broadcast/multi-source-broadcasting",children:"Multisource stream"}),", you can open the stream viewer from the ",(0,a.jsx)(i.a,{href:"https://streaming.dolby.io/#/tokens",children:"Dolby.io dashboard"})," or by navigating to:"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{children:"https://viewer.millicast.com?streamId=[YOUR_ACCOUNT_ID]/[YOUR_STREAM_NAME]\n"})}),"\n",(0,a.jsx)(i.p,{children:"Once you join, the bottom right gear icon flashes a notification prompting you to enable multi-view. Enable it to begin viewing the streams."}),"\n","\n",(0,a.jsx)("div",{class:"center-container",children:(0,a.jsx)("img",{src:s.A,width:"600"})}),"\n",(0,a.jsxs)(i.p,{children:["Alternatively, you can force the viewer to open to multi-view by including the ",(0,a.jsx)(i.code,{children:"&multisource=true"})," flag on the URL:"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{children:"https://viewer.millicast.com?streamId=[YOUR_ACCOUNT_ID]/[YOUR_STREAM_NAME]&multisource=true\n"})}),"\n",(0,a.jsx)(i.h2,{id:"creating-a-multi-view-web-application",children:"Creating a Multi-view web application"}),"\n",(0,a.jsxs)(i.p,{children:["Dolby.io supports ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/source-and-layer-selection",children:"Multisource Playback"})," via the ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/players-sdks/",children:"Client SDKs"}),", allowing you to build your own multi-view experience for your app or platform."]}),"\n",(0,a.jsx)(i.p,{children:"Before getting started building a multi-view application it is worth understanding;"}),"\n",(0,a.jsxs)(i.ol,{children:["\n",(0,a.jsxs)(i.li,{children:["How to broadcast ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/broadcast/multi-source-broadcasting",children:"Multisource Streams"}),"."]}),"\n",(0,a.jsxs)(i.li,{children:["How to ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/getting-started/creating-real-time-streaming-web-app",children:"Create a Basic Streaming Web App"}),"."]}),"\n",(0,a.jsxs)(i.li,{children:["What ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/players-sdks/viewer-events",children:"Broadcast Events"})," are and how to use them."]}),"\n",(0,a.jsxs)(i.li,{children:["How the Dolby.io platform organizes and handles ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/source-and-layer-selection",children:"Multisource Playback"}),"."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"store-and-track-incoming-multisource-feeds",children:"Store and track incoming Multisource feeds"}),"\n",(0,a.jsx)(i.admonition,{title:"Not building a Web App?",type:"info",children:(0,a.jsxs)(i.p,{children:["All Dolby.io ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/players-sdks/",children:"Client SDKs"})," support building Multi-view applications. Although the below example is using JavaScript the principles are the same for each SDK."]})}),"\n",(0,a.jsxs)(i.p,{children:["The Dolby.io platform tracks broadcasts by their ",(0,a.jsx)(i.code,{children:"account ID"})," and ",(0,a.jsx)(i.code,{children:"stream name"})," and individual streams within broadcasts by their ",(0,a.jsx)(i.code,{children:"sourceID"}),", a unique identifier that can be used for selecting feeds to render from the ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/source-and-layer-selection",children:"viewer node"}),". Unlike a traditional broadcast where there is only one stream to playback, a multi-view application must account for multiple feeds arriving asynchronously. To accomplish this, the application should ",(0,a.jsxs)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/players-sdks/viewer-events#using-events",children:["listen for streams using a ",(0,a.jsx)(i.code,{children:"broadcastEvent"})]}),", and store the stream ",(0,a.jsx)(i.code,{children:"sourceID"})," as it becomes active."]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-javascript",children:"const activeSources = new Set();\n\nawait millicastView.on('broadcastEvent', (event) => {\n  const { name, data } = event;\n\n  if (name === 'active') {\n    // Add the new source ID to the list of active sources\n    activeSources.add(data.sourceID);\n  }\n});\n"})}),"\n",(0,a.jsx)(i.h3,{id:"add-video-elements-and-render-feeds",children:"Add video elements and render feeds"}),"\n",(0,a.jsxs)(i.p,{children:["Once we've captured the ",(0,a.jsx)(i.code,{children:"sourceID"})," of an incoming stream, we need to signal to the Viewer node which ",(0,a.jsx)(i.em,{children:"track"})," the stream will play on. The Dolby.io Millicast SDKs include a function that allows you to ",(0,a.jsx)(i.a,{href:"/documentation/pr-preview/pr-505/millicast/playback/source-and-layer-selection#dynamic-viewer-track",children:"dynamically add a track to the Viewer node"})," called ",(0,a.jsx)(i.code,{children:"addRemoteTrack"}),"."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.a,{href:"https://millicast.github.io/millicast-sdk/View.html#addRemoteTrack",children:"addRemoteTrack"})," requires the media type of the incoming stream (",(0,a.jsx)(i.em,{children:"audio or video"}),") and a ",(0,a.jsx)(i.a,{href:"https://developer.mozilla.org/en-US/docs/Web/API/MediaStream",children:(0,a.jsx)(i.code,{children:"MediaStream"})}),", an interface that signals a stream of media content. ",(0,a.jsx)(i.code,{children:"addRemoteTrack"})," will then return a promise that will be resolved when the ",(0,a.jsx)(i.a,{href:"https://developer.mozilla.org/en-US/docs/Web/API/RTCRtpTransceiver",children:(0,a.jsx)(i.code,{children:"RTCRtpTransceiver"})})," is assigned a ",(0,a.jsx)(i.code,{children:"mid"})," value."]}),"\n",(0,a.jsxs)(i.p,{children:["These newly created ",(0,a.jsx)(i.em,{children:"Transceivers"})," can be stored alongside the ",(0,a.jsx)(i.code,{children:"sourceID"}),", ready for when it is time to render the feed in the app."]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-javascript",children:"const sourceIdTransceiversMap = new Map();\n\nconst addStreamToYourVideoTag = async (data) => {\n  const mediaStream = new MediaStream();\n  const videoTransceiver = await millicastView.addRemoteTrack('video', [mediaStream]);\n  const audioTransceiver = await millicastView.addRemoteTrack('audio', [mediaStream]);\n\n  const videoMediaId = tracksMapping.find((track) => data.track.media === 'video').mediaId;\n  const audioMediaId = tracksMapping.find((track) => data.track.media === 'audio')?.mediaId;\n\n  sourceIdTransceiversMap.set(data.sourceId, {\n    videoMediaId,\n    audioMediaId,\n  });\n  createVideoElement(mediaStream, data.sourceId);\n};\n"})}),"\n",(0,a.jsxs)(i.p,{children:["To actually add the stream to the ",(0,a.jsx)(i.code,{children:"<video>"})," tag we must create the ",(0,a.jsx)(i.code,{children:"<video>"})," element and assign it the ",(0,a.jsx)(i.code,{children:"mediaStream"})," created above. To disambiguate which ",(0,a.jsx)(i.code,{children:"<video>"})," element holds which stream we can assign the element the ",(0,a.jsx)(i.code,{children:"sourceID"}),"."]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-javascript",children:"const createVideoElement = (mediaStream, sourceID) => {\n  const videoElements = document.getElementById('videoElements');\n\n  const video = document.createElement('video');\n  video.id = sourceID;\n  video.srcObject = mediaStream;\n  video.autoplay = true;\n  video.muted = true;\n\n  videoElements.appendChild(video);\n};\n"})}),"\n",(0,a.jsxs)(i.p,{children:["In the above code ",(0,a.jsx)(i.code,{children:"videoDiv"})," is where we want the ",(0,a.jsx)(i.code,{children:"<video>"})," tag to show up in the HTML. Hence, at the end of the function, we append the newly created ",(0,a.jsx)(i.code,{children:"<video>"})," element to the ",(0,a.jsx)(i.code,{children:"<div>"}),"."]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"To recap, we've:"})}),"\n",(0,a.jsxs)(i.ol,{children:["\n",(0,a.jsxs)(i.li,{children:["Captured the ",(0,a.jsx)(i.code,{children:"sourceID"})," of an incoming stream by listening for an ",(0,a.jsx)(i.code,{children:"active"})," ",(0,a.jsx)(i.code,{children:"broadcastEvent"}),"."]}),"\n",(0,a.jsxs)(i.li,{children:["Created a new ",(0,a.jsx)(i.code,{children:"mediaStream"})," object and used it to ",(0,a.jsx)(i.code,{children:"addRemoteTrack"})," to the Viewer node."]}),"\n",(0,a.jsxs)(i.li,{children:["Mapped the newly created ",(0,a.jsx)(i.code,{children:"mediaStream"})," called a ",(0,a.jsx)(i.code,{children:"Transceiver"})," to the newly captured ",(0,a.jsx)(i.code,{children:"sourceID"}),"."]}),"\n",(0,a.jsxs)(i.li,{children:["Created a ",(0,a.jsx)(i.code,{children:"<video>"})," element and associate that element with the newly created media."]}),"\n",(0,a.jsxs)(i.li,{children:["Added the ",(0,a.jsx)(i.code,{children:"<video>"})," element and its ",(0,a.jsx)(i.code,{children:"mediaStream"})," to the ",(0,a.jsx)(i.code,{children:"<div>"})," where it will render."]}),"\n"]}),"\n",(0,a.jsxs)(i.p,{children:["At this stage, all the pieces are together, however, the stream won't yet render. This is because you have yet to tell the Dolby.io Viewer node which stream to project onto the ",(0,a.jsx)(i.code,{children:"Transceiver"}),". This is done using the ",(0,a.jsxs)(i.a,{href:"/millicast/playback/source-and-layer-selection",children:[(0,a.jsx)(i.code,{children:"project"})," function"]})," which tells the node to begin ",(0,a.jsx)(i.em,{children:"projecting"})," the stream, identified by its ",(0,a.jsx)(i.code,{children:"sourceID"}),", onto the ",(0,a.jsx)(i.code,{children:"Transceiver"}),"."]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-javascript",children:"await millicastView.project(sourceID, [\n  {\n    mediaId: videoTransceiver.mid,\n    media: 'video',\n  },\n  {\n    mediaId: audioTransceiver.mid,\n    media: 'audio',\n  },\n]);\n"})}),"\n",(0,a.jsxs)(i.p,{children:["Once projected the stream will begin playing within the ",(0,a.jsx)(i.code,{children:"<video>"})," tag."]}),"\n",(0,a.jsx)(i.p,{children:"Put all together, a basic multi-view application would look something like this:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-javascript",children:"// Authenticate a Connection to the Dolby.io CDN\nconst tokenGenerator = () =>\n  millicast.Director.getSubscriber({\n    streamName: 'YOUR STREAM NAME',\n    streamAccountId: 'YOUR ACCOUNT ID',\n  });\n\n// Connect to the Viewer node\nconst millicastView = new millicast.View(undefined, tokenGenerator);\n\nconst activeSources = new Set();\n\nmillicastView.on('broadcastEvent', async (event) => {\n  const { name, data } = event;\n\n  if (name === 'active') {\n    // Add the new source ID to the list of active sources\n    activeSources.add(data.sourceId);\n    await addStreamToYourVideoTag(data);\n  }\n});\n\n// Create and add video streams with sourceId\nconst addStreamToYourVideoTag = async (data) => {\n  const mediaStream = new MediaStream();\n  const tracksMapping = [];\n\n  // Look for the audio track\n  const trackAudio = data.tracks.find(({ media }) => media === 'audio');\n  if (trackAudio) {\n    const audioTransceiver = await viewer.addRemoteTrack('audio', [mediaStream]);\n    tracksMapping.push({\n      media: 'audio',\n      mediaId: audioTransceiver?.mid,\n    });\n  }\n\n  // Look for the video track\n  const trackVideo = data.tracks.find(({ media }) => media === 'video');\n  if (trackVideo) {\n    const videoTransceiver = await viewer.addRemoteTrack('video', [mediaStream]);\n    tracksMapping.push({\n      media: 'video',\n      mediaId: videoTransceiver?.mid,\n    });\n  }\n\n  createVideoElement(mediaStream, data.sourceId);\n\n  await millicastView.project(sourceId, tracksMapping);\n};\n\n// Add video stream to video element\nconst createVideoElement = (mediaStream, sourceId) => {\n  const video = document.createElement('video');\n  video.id = sourceId;\n  video.srcObject = mediaStream;\n\n  document.getElementById('videoDiv').appendChild(video);\n};\n"})}),"\n",(0,a.jsx)(i.h3,{id:"final-result",children:"Final result"}),"\n",(0,a.jsxs)(i.p,{children:["Additional features can be added, such as the ability to remove feeds once they stop or a button to switch between feeds. To learn more, explore this ",(0,a.jsx)(i.a,{href:"https://optiview.dolby.com/resources/blog/streaming/building-a-webrtc-live-stream-multiviewer-app/",children:"full guide on building a multi-view application from start to finish"})," or try it out yourself with this ",(0,a.jsx)(i.a,{href:"https://github.com/dolbyio-samples/stream-app-web-viewer/tree/Multiviewer",children:"working sample code"}),"."]}),"\n",(0,a.jsx)("video",{autoplay:"",width:"800px",controls:"",loop:"",muted:"",src:"https://dolby.io/wp-content/uploads/2022/11/multiview-vid.mp4",playsinline:""}),"\n",(0,a.jsx)(i.h2,{id:"assigning-lower-quality-layers-to-small-tiles",children:"Assigning lower-quality layers to small tiles"}),"\n",(0,a.jsx)(i.p,{children:"By allocating lower-quality layers to smaller video tiles, you can optimize bandwidth usage and ensure a smoother streaming experience. Small tiles may not require high-resolution details, so using lower-quality layers conserves resources and enables efficient distribution of the available bandwidth."}),"\n",(0,a.jsxs)(i.p,{children:["Start by creating a ",(0,a.jsx)(i.code,{children:"transceiverToSourceIdMap"})," variable to associate the media IDs with corresponding source IDs. After establishing a successful stream connection, listen to the ",(0,a.jsx)(i.code,{children:"layers"})," ",(0,a.jsx)(i.a,{href:"https://millicast.github.io/millicast-sdk/Signaling.html#event:broadcastEvent",children:"broadcastEvent"})," that is triggered whenever the state of layers in the live stream is updated. Iterate through the media IDs in the created variable; when an ID is not equal to 0, project the lowest layer."]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-javascript",children:"const updateLayers = async (layers) => {\n  // Iterate the current mapping of media ID sources\n  for (var mid in transceiverToSourceIdMap) {\n    // If not main source (mid is set to 0 by default)\n    if (mid !== '0') {\n      //Order the layers to get the lowest one and project it\n      const lowerLayer = layers[mid].active.reduce((currentLower, currentValue) => {\n        return currentValue.bitrate < currentLower.bitrate ? currentValue : currentLower;\n      });\n\n      await viewer.project(transceiverToSourceIdMap[mid], [\n        {\n          mediaId: mid,\n          layer: { encodingId: lowerLayer.id },\n          media: 'video',\n        },\n      ]);\n    }\n  }\n};\n"})}),"\n",(0,a.jsx)(i.h2,{id:"limitations-of-multi-view",children:"Limitations of Multi-view"}),"\n",(0,a.jsxs)(i.p,{children:["Dolby.io Real-time Streaming does not limit the number of tracks that a viewer can receive, however, it limits the aggregate bitrate of all tracks to 12 Mbps. The pinned source is prioritized and allowed to exceed the 12 Mbps limit, and the other tracks share any remaining available bandwidth. The source with a null ",(0,a.jsx)(i.code,{children:"sourceId"})," is pinned by default. You can change the pinned source by using the ",(0,a.jsx)(i.code,{children:"pinnedSourceId"})," attribute in the ",(0,a.jsx)(i.code,{children:"View.connect"})," command. You should configure the Simulcast/SVC bitrate of each source so that a viewer can receive the desired amount of video tracks in the viewer session while remaining under the aggregate bitrate limit."]}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{style:{textAlign:"left"},children:"Example"}),(0,a.jsx)(i.th,{style:{textAlign:"left"},children:"Bandwidth Allocation"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"A 4 Mbps pinned track and four simulcast tracks"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"4 Mbps is allocated to the pinned track and the other simulcast tracks receive 2 Mbps each."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"A 4 Mbps pinned track and two 2 Mbps tracks"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"The overall bitrate is is under the 12 Mbps limit."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"A 12 Mbps pinned track and four simulcast tracks"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"12 Mbps is allocated to the pinned track and other tracks receive no bandwidth"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"A 10 Mbps pinned track and two 2 Mbps tracks"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"10 Mbps is allocated to the pinned track and there is only space for one additional track"})]})]})]})]})}function m(e={}){let{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},778093(e,i,t){t.d(i,{A:()=>n});let n=t.p+"assets/images/Select_multiview-d618957a6283fe804c251648207a2c2a.jpg"},28453(e,i,t){t.d(i,{R:()=>s,x:()=>o});var n=t(296540);let a={},r=n.createContext(a);function s(e){let i=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),n.createElement(r.Provider,{value:i},e.children)}},585501(e){e.exports=JSON.parse('{"id":"playback/multi-view","title":"Multi-view","description":"Multi-view lets you ingest and render multiple Dolby.io real-time video and audio streams simultaneously inside a browser or mobile native applications. Once rendered, you can switch seamlessly between streams, allowing you to control how you view the content. By giving viewers content control, broadcasters can enable real-time experiences and engagement that leave viewers wanting more.","source":"@site/millicast/playback/multi-view.md","sourceDirName":"playback","slug":"/playback/multiview","permalink":"/documentation/pr-preview/pr-505/millicast/playback/multiview","draft":false,"unlisted":false,"editUrl":"https://github.com/THEOplayer/documentation/blob/-/millicast/playback/multi-view.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Multi-view","slug":"/playback/multiview","sidebar_position":4},"sidebar":"millicast","previous":{"title":"Source and Layer Selection","permalink":"/documentation/pr-preview/pr-505/millicast/playback/source-and-layer-selection"},"next":{"title":"Audio Multiplexing","permalink":"/documentation/pr-preview/pr-505/millicast/playback/audio-multiplexing"}}')}}]);