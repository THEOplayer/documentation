"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([["46754"],{149287:function(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>a,toc:()=>u});var a=i(982849),t=i(785893),r=i(250065);let s={title:"Ingest Raw Frames",sidebar_position:11},o=void 0,l={},u=[{value:"Using the Core API",id:"using-the-core-api",level:2},{value:"Creating custom audio and video frame classes",id:"creating-custom-audio-and-video-frame-classes",level:3},{value:"Creating and publishing a custom source",id:"creating-and-publishing-a-custom-source",level:3},{value:"Using the iOS API",id:"using-the-ios-api",level:2},{value:"Creating custom audio and video frame classes",id:"creating-custom-audio-and-video-frame-classes-1",level:3},{value:"Creating and publishing a custom source",id:"creating-and-publishing-a-custom-source-1",level:3},{value:"Using the Android API",id:"using-the-android-api",level:2},{value:"Creating custom audio and video frame classes",id:"creating-custom-audio-and-video-frame-classes-2",level:3},{value:"Creating and publishing a custom source",id:"creating-and-publishing-a-custom-source-2",level:3}];function d(e){let n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.p,{children:["Ingesting raw video and audio frames can be useful for creating and managing unique WebRTC encoding workflows. Whilst ",(0,t.jsx)(n.a,{href:"/documentation/pr-preview/pr-369/millicast/hardware-encoders",children:"traditional encoders"})," provide general-purpose encoding solutions, there are some use cases such as ",(0,t.jsx)(n.a,{href:"/documentation/pr-preview/pr-369/millicast/capture/live-streaming-from-drones",children:"real-time streaming from drones"})," where traditional encoding solutions are too heavy, power consumptive, or expensive to suffice."]}),"\n",(0,t.jsx)(n.p,{children:"This guide is designed to help you leverage the Dolby.io Streaming Native SDKs to ingest raw audio and video frames, allowing the stream encoding to be handled by the SDK. There are three ways to accomplish raw frame ingestion:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#creating-custom-audio-and-video-frame-classes",children:"Using the Core API"}),": For desktop applications such as Windows, Mac, or Linux"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#using-the-ios-api",children:"Using the iOS API"}),": Including tvOS"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#using-the-android-api",children:"Using the Android API"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"using-the-core-api",children:"Using the Core API"}),"\n",(0,t.jsx)(n.h3,{id:"creating-custom-audio-and-video-frame-classes",children:"Creating custom audio and video frame classes"}),"\n",(0,t.jsxs)(n.p,{children:["To ingest raw frames, you first must download and install the appropriate ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk/releases",children:"Millicast Native SDK"}),". Before you begin using the SDK, it is recommended you review the list of ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#millicast-native-sdk",children:"supported operating systems"}),", ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#audio-codecs-and-quality---encoding",children:"audio encoding limitations"}),", and ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#video-codecs-and-quality---encoding",children:"video encoding limitations"})," as various limitations may impact your project. Additionally, if you encounter a bug, please ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk/issues",children:"open an issue"})," on the native project."]}),"\n",(0,t.jsx)(n.p,{children:"To get started providing custom video and audio frames, we will first create a video frame class that represents a static RGB color and an audio frame class that generates a SINE tone."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:"#include <vector>\n#include <cmath>\n#include <millicast-sdk/frames.h>\n\n/**\n We are required to implement the millicast::VideoFrame interface with our\n own logic. Realistically, this class should be more sophisticated in that\n it takes an input your raw frame buffers, but for now, we generate a static\n frame of one RGB color passed as input.\n*/\nclass CustomVideoFrame: public millicast::VideoFrame {\nprivate:\n    int _r, _g, _b;\n    static constexpr auto WIDTH = 640;\n    static constexpr auto HEIGHT = 480;\npublic:\n  CustomVideoFrame(int r, int g, int b) : _r(r), _g(g), _b(b) {}\n\n  int width() const override { return WIDTH; }\n  int height() const override { return HEIGHT; }\n\n  /**\n    Override this to return the corresponding frame type to the frame buffer you store\n    within the frame buffer. VideoType values supported can be found here:\n    https://millicast.github.io/doc/latest/cpp/namespacemillicast.html#a3e878ddbbd034e20ba1b96575ac0fd2a\n  */\n  millicast::VideoType frame_type() const override { return millicast::VideoType::ARGB; }\n\n  millicast::PrimaryID primaries() const override { return {}; }\n  millicast::TransferID transfer() const override { return {}; }\n  millicast::MatrixID matrix() const override { return {}; }\n  millicast::RangeID range() const override { return {}; }\n\n  /**\n    This should return the size of the frame buffer, and depends on the frame type. You only need\n    to return the size for the frame type that you support in this class. For example, the formula\n    for calculating the size of an I420 frame is frame_size = width * height * 3 / 2 .\n  */\n  uint32_t size(millicast::VideoType type) const override { return width() * height() * 4; };\n\n  /**\n    Buffer is pre-allocated based on size. You are required to fill\n    the provided buffer with the frames you have, which should also correspond\n    to the frame_type() you defined earlier. For example, if you would like to\n    pass I420 frames, then frame_type() should return millicast::VideoType::I420.\n  */\n  void get_buffer(millicast::VideoType type, uint8_t *buffer) const override\n  {\n    // We will always request the type to be the same you declared in frame_type,\n    // so no need to worry about other frame types.\n\n    using namespace millicast;\n\n    if (!buffer) return;\n    std::memset(buffer, 0, size(type));\n\n    for (int i = 0; i < size(frame_type()); i += 4)\n    {\n      buffer[i] = 255; // This is the alpha channel which WebRTC does not use.\n      buffer[i + 1] = _r;\n      buffer[i + 2] = _g;\n      buffer[i + 3] = _b;\n    }\n  }\n};\n\n\nclass CustomSineAudioFrame {\nprivate:\n    std::vector<float> _audio_data;\n\n    const int _sample_rate{};\n    const size_t _num_channels{}, _chunk_time{};\n\n\n    size_t _sini{0};\n\n    static constexpr auto PI = 3.14159264;\n    static constexpr auto TONE_FREQUENCY = 480; // 480Hz\npublic:\n    /**\n      chunk_time has to be in milliseconds.\n      sample_rate has to be in Hz\n    */\n    CustomSineAudioFrame(\n        int sample_rate,\n        size_t num_channels,\n        size_t chunk_time)\n    : _sample_rate(sample_rate),\n      _num_channels(num_channels),\n      _chunk_time(chunk_time) {\n\n        // Calculate buffer size, and resize accordingly.\n        const auto buffer_size = num_channels * sample_rate * chunk_time / 1000;\n        _audio_data.resize(buffer_size);\n    }\n\n    millicast::AudioFrame generate_sine_audio_frame() {\n      millicast::AudioFrame frame;\n      frame.bits_per_sample = sizeof(float) * 8;\n      frame.number_of_channels = _num_channels;\n      frame.sample_rate = _sample_rate;\n      frame.number_of_frames = _sample_rate * _chunk_time / 1000;\n\n\n\n      for(int j = 0; j < _audio_data.size(); j+=_num_channels) {\n        float value = sin(_sini * TONE_FREQUENCY * 2.f * PI / _sample_rate);\n        for(int k = 0; k < _num_channels; k++) _audio_data[j+k] = value;\n\n        // Our cycle\n        _sini = (++_sini) % (_sample_rate / TONE_FREQUENCY);\n      }\n\n      frame.data = _audio_data.data();\n      return frame;\n    }\n};\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The video frame class is essentially generating a static RGB frame every time ",(0,t.jsx)(n.code,{children:"CustomVideoFrame::get_buffer"})," gets called. For your scenario, you should create your video frame class so that it takes your raw frame data as input, as well as any other important information, like the size and pixel format. ",(0,t.jsx)(n.code,{children:"VideoFrame::get_buffer"})," should fill the provided buffer, which is pre-allocated before calling the function, with data that is of the same frame type as what is returned by ",(0,t.jsx)(n.code,{children:"VideoFrame::get_type"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["For audio, you need to instantiate a ",(0,t.jsx)(n.code,{children:"millicast::AudioFrame"})," object, and fill that object with the following information:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A pointer to the audio data (for which the SDK holds no ownership and needs to be managed)"}),"\n",(0,t.jsx)(n.li,{children:"The bit depth (or bits per sample)"}),"\n",(0,t.jsx)(n.li,{children:"The number of channels"}),"\n",(0,t.jsxs)(n.li,{children:["The sample rate, ",(0,t.jsx)(n.strong,{children:"which must be 48 kHz"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["The number of frames. This is dependent on the length of each frame. For example, if each frame equates to 10 milliseconds, then the number of frames should be ",(0,t.jsx)(n.code,{children:"10 ms * 48 kHz = 480 frames"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"With the information above, make sure your audio data buffer's size is equal to the following:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:"CHUNK_SIZE = 10 // milliseconds. this is an example length of each frame\nNUMBER_OF_FRAMES = CHUNK_SIZE * SAMPLE_RATE\nBUFFER_SIZE = NUMBER_OF_FRAMES * (BIT_DEPTH / 8) * NUMBER_OF_CHANNELS // in bytes.\n"})}),"\n",(0,t.jsxs)(n.p,{children:["In this specific example, we have created a ",(0,t.jsx)(n.code,{children:"CustomSineAudioFrame"})," class that generates Sine audio at a specific frequency."]}),"\n",(0,t.jsx)(n.h3,{id:"creating-and-publishing-a-custom-source",children:"Creating and publishing a custom source"}),"\n",(0,t.jsx)(n.p,{children:"Now that we have our Audio/Video frame classes ready, we can start instantiating an audio and video source:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:"#include <chrono>\n#include <thread>\n\n#include <millicast-sdk/client.h>\n#include <millicast-sdk/media.h>\n#include <millicast-sdk/publisher.h>\n\n//... Include the frame classes here...\n\nvoid create_custom_source_and_publish() {\n  auto vsource = millicast::CustomVideoSource::Builder{}.build();\n  auto asource = millicast::CustomAudioSource::Builder{}.build();\n\n  // Those tracks can now be added just as any other track to our publisher.\n  // (Refer to the other documentation to learn how to perform a simple\n  // publishing scenario)\n  auto video_track = vsource->start_capture();\n  auto audio_track = asource->start_capture();\n\n  // Create publisher and add tracks here ...\n\n  // Start two simple threads to generate audio and video frames\n  std::thread audio_thread([asource]() {\n    constexpr auto SAMPLE_RATE = 48000;  // 48kHz\n    constexpr auto NUM_CHANNELS = 2;     // Stereo\n    constexpr auto CHUNK_TIME = 10;  // Each sample equates to 10ms of playtime.\n    constexpr auto TOTAL_TRACK_LENGTH =\n        25000;  // 25 seconds of playtime (arbitrary length)\n\n    CustomSineAudioFrame frame_generator(SAMPLE_RATE, NUM_CHANNELS, CHUNK_TIME);\n\n    for (auto i = 0; i < TOTAL_TRACK_LENGTH / CHUNK_TIME; i++) {\n      millicast::AudioFrame audio_frame =\n          frame_generator.generate_sine_audio_frame();\n      asource->on_audio_frame(audio_frame);\n      std::this_thread::sleep_for(std::chrono::milliseconds(CHUNK_TIME));\n    }\n  });\n\n  std::thread video_thread([vsource]() {\n    constexpr auto FPS = 60;                  // 60 frames per second\n    constexpr auto NUM_FRAMES = 60 * 25;      // 25 seconds of playtime\n    CustomVideoFrame video_frame(255, 0, 0);  // Just a red frame\n\n    for (auto i = 0; i < NUM_FRAMES; i++) {\n      vsource->on_video_frame(video_frame);\n      std::this_thread::sleep_for(std::chrono::milliseconds(1000 / FPS));\n    }\n  });\n\n  // Connect and publish here ...\n\n  // When done, stop the capture\n  asource->stop_capture();\n  vsource->stop_capture();\n\n  // Make sure to join the threads here\n  audio_thread.join();\n  video_thread.join();\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:["With the audio and video source instantiated, the frames can now be passed as inputs to the ",(0,t.jsx)(n.code,{children:"Publisher"})," object. To learn more about publishing a stream from the native SDKs explore the ",(0,t.jsx)(n.a,{href:"/documentation/pr-preview/pr-369/millicast/playback/players-sdks/",children:"Native SDK guides"})," in the Client SDKs section of the documentation."]}),"\n",(0,t.jsx)(n.h2,{id:"using-the-ios-api",children:"Using the iOS API"}),"\n",(0,t.jsx)(n.h3,{id:"creating-custom-audio-and-video-frame-classes-1",children:"Creating custom audio and video frame classes"}),"\n",(0,t.jsxs)(n.p,{children:["To ingest raw frames, you first must download and install the appropriate ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk/releases",children:"Millicast Native SDK"}),". Before you begin using the SDK, it is recommended you review the list of ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#millicast-native-sdk",children:"supported operating systems"}),", ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#audio-codecs-and-quality---encoding",children:"audio encoding limitations"}),", and ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#video-codecs-and-quality---encoding",children:"video encoding limitations"})," as various limitations may impact your project. Additionally, if you encounter a bug, please ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk/issues",children:"open an issue"})," on the native project."]}),"\n",(0,t.jsx)(n.p,{children:"Similarly to the Core API, in Swift, we need to create a custom video frame class that represents a static RGB color and an audio frame class that generates a SINE tone."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-swift",children:"import MillicastSDK\n\nclass CustomVideoFrame: NSObject, MCVideoFrame {\n\n  /**\n  	This should return the correct frame type to what you\n  	store in this frame buffer. Video types supported can be found\n		here: https://millicast.github.io/doc/latest/cpp/objc_2_millicast_s_d_k_2capabilities_8h.html#ac37bb3c55eced5c69ad84759f23e6c90\n  */\n  func frameType() -> MCVideoType {\n    return ARGB;\n  }\n\n  static let WIDTH: Int32 = 640;\n  static let HEIGHT: Int32 = 480;\n\n  var _r, _g, _b: UInt8;\n\n  init(_r: UInt8, _g: UInt8, _b: UInt8) {\n    self._r = _r\n    self._g = _g\n    self._b = _b\n  }\n\n  func width() -> Int32 {\n    return CustomVideoFrame.WIDTH;\n  }\n\n  func height() -> Int32 {\n    return CustomVideoFrame.HEIGHT;\n  }\n\n  /**\n  	This should return the size of the frame buffer if your frame is an RGB frame.\n  */\n  func sizeRgb() -> UInt32 {\n    return UInt32(width() * height() * 4);\n  }\n\n  /**\n    This should return the size of the frame buffer if your frame is an I420 frame. For example, the formula\n    for calculating the size of an I420 frame is frame_size = width * height * 3 / 2 .\n		Ignore this if your frame is not I420.\n  */\n  func sizeI420() -> UInt32 {\n    return 0;\n  }\n\n  /**\n    Implementing as empty since our frame is RGB.\n  */\n  func sizeI444() -> UInt32 {\n    return 0;\n  }\n\n  /**\n    Implementing as empty since our frame is RGB.\n  */\n  func getI444Buffer(_ buffer: UnsafeMutablePointer<UInt8>!) {\n    return;\n  }\n\n  /**\n    If your frame buffer is I420, you should implement this instead.\n		You need to fill the provided buffer with your frame buffer.\n  */\n  func getI420Buffer(_ buffer: UnsafeMutablePointer<UInt8>!) {\n    return;\n  }\n\n\n  func getRgbBuffer(_ buffer: UnsafeMutablePointer<UInt8>!) {\n    let size = Int(sizeRgb());\n    for i in stride(from: 0, to: size - 1, by: 4) {\n      buffer[i] = 255; // Alpha-channel\n      buffer[i + 1] = _r;\n      buffer[i + 2] = _g;\n      buffer[i + 3] = _b;\n    }\n  }\n\n  func getI420Buffer(_ buffer: UnsafeMutablePointer<UInt8>!) {\n	// If your frame type is I420, you need to fill the buffer\n	// here instead.\n  }\n}\n\nclass CustomSineAudioFrame {\n\n  var _audioData: UnsafeMutablePointer<Float32>;\n  var _bufferSize: Int;\n\n  var _sampleRate: Int;\n  var _numChannels, _chunkTime: Int;\n\n  var _sini: Int;\n\n  static let PI: Float = 3.14159264;\n  static let TONE_FREQUENCY: Int = 480 // 480 Hz\n\n  init(_sampleRate: Int, _numChannels: Int, _chunkTime: Int) {\n    self._sini = 0;\n    self._sampleRate = _sampleRate;\n    self._numChannels = _numChannels;\n    self._chunkTime = _chunkTime;\n\n    self._bufferSize = _numChannels * _sampleRate * _chunkTime / 1000;\n    self._audioData = UnsafeMutablePointer<Float32>.allocate(capacity: self._bufferSize);\n  }\n\n  func generate_sine_audio_frame() -> MCAudioFrame {\n    var frame: MCAudioFrame = MCAudioFrame();\n    frame.bitsPerSample = 32; // We are using Float32\n    frame.channelNumber = self._numChannels;\n    frame.sampleRate = Int32(self._sampleRate);\n    frame.frameNumber = Int(self._sampleRate * self._chunkTime / 1000);\n\n    for j in stride(from:0, to: self._bufferSize, by: self._numChannels) {\n      let constant = 2.0 * Float(CustomSineAudioFrame.TONE_FREQUENCY) * CustomSineAudioFrame.PI;\n      let x: Float = Float(_sini) *  constant / Float(_sampleRate);\n      let value: Float = sin(x);\n      for k in 0...Int(self._numChannels) - 1 {\n        self._audioData[j+k] = value;\n      }\n\n      // Our cycle\n      self._sini += 1 ;\n      self._sini %= (self._sampleRate / CustomSineAudioFrame.TONE_FREQUENCY);\n    }\n\n    frame.data = UnsafeRawPointer(self._audioData);\n\n    return frame;\n  }\n\n  deinit {\n    self._audioData.deallocate();\n  }\n\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:"We also will create threads in Swift to handle feeding the source with raw audio and video frames:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-swift",children:"class VideoThread: Thread {\n\n  var _source: MCCoreVideoSource;\n  let waiter = DispatchGroup()\n\n  static let FPS: Int = 60; // Running at 60 frames per second\n  static let DURATION: Int = 25 // 25 seconds of playtime per thread\n\n  init(_source: MCCoreVideoSource) {\n    self._source = _source;\n    super.init();\n  }\n\n  override func start() {\n      waiter.enter()\n      super.start()\n  }\n\n  override func main() {\n      task()\n      waiter.leave()\n  }\n\n  func task() {\n    var pixelBuffer: CVPixelBuffer? = nil\n    CVPixelBufferCreate(kCFAllocatorDefault, 1280, 720, kCVPixelFormatType_32BGRA, nil, &pixelBuffer);\n    for _ in 0...Int(VideoThread.FPS * VideoThread.DURATION) {\n      _source.onPixelBuffer(pixelBuffer);\n      Thread.sleep(forTimeInterval:1.0/Double(VideoThread.FPS));\n    }\n  }\n\n  func join() {\n      waiter.wait()\n  }\n}\n\nclass AudioThread: Thread {\n\n  var _source: MCCustomAudioSource;\n  let waiter = DispatchGroup()\n\n  static let SAMPLE_RATE = 48000; // 48kHz\n  static let NUM_CHANNELS = 2; // Stereo\n  static let CHUNK_TIME = 10; // Each sample equates to 10ms of playtime.\n  static let TOTAL_TRACK_LENGTH = 25000; // 25 seconds of playtime (arbitrary length)\n\n  init(_source: MCCustomAudioSource) {\n    self._source = _source;\n    super.init();\n  }\n\n  override func start() {\n      waiter.enter()\n      super.start()\n  }\n\n  override func main() {\n      task()\n      waiter.leave()\n  }\n\n  func task() {\n    var frame_generator: CustomSineAudioFrame = CustomSineAudioFrame(\n      _sampleRate: AudioThread.SAMPLE_RATE,\n      _numChannels: AudioThread.NUM_CHANNELS,\n      _chunkTime: AudioThread.CHUNK_TIME);\n\n    let duration: Int = Int(AudioThread.TOTAL_TRACK_LENGTH / AudioThread.CHUNK_TIME);\n    for _ in 0...duration {\n      _source.onAudioFrame(frame_generator.generate_sine_audio_frame());\n      Thread.sleep(forTimeInterval:Double(AudioThread.CHUNK_TIME)/1000.0);\n    }\n  }\n\n  func join() {\n      waiter.wait()\n  }\n}\n"})}),"\n",(0,t.jsx)(n.h3,{id:"creating-and-publishing-a-custom-source-1",children:"Creating and publishing a custom source"}),"\n",(0,t.jsx)(n.p,{children:"As with the Core API example, the only thing left now is to instantiate the custom sources, add them to the publisher, and start up the threads:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-swift",children:"func createCustomSourceAndPublish() {\n  var vsource: MCCoreVideoSource = MCCoreVideoSourceBuilder().build();\n  var asource: MCCustomAudioSource = MCCustomAudioSourceBuilder().build();\n\n  // Those tracks can now be added just as any other track to our publisher.\n  // (Refer to the other documentation to learn how to perform a simple publishing scenario)\n  var video_track = vsource.startCapture();\n  var audio_track = asource.startCapture();\n\n  // Create publisher and add tracks here ...\n\n  // Create and start threads\n  var videoThread: VideoThread = VideoThread(_source: vsource);\n  var audioThread: AudioThread = AudioThread(_source: asource);\n  videoThread.start();\n  audioThread.start();\n\n  // Connect and publish here ...\n\n\n  // Once done, stop the capture\n  asource.stopCapture();\n  vsource.stopCapture();\n\n  // Join the threads\n  videoThread.join();\n  audioThread.join();\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:["With the audio and video source instantiated, the frames can now be passed as inputs to the ",(0,t.jsx)(n.code,{children:"Publisher"})," object. To learn more about publishing a stream from the native SDKs explore the ",(0,t.jsx)(n.a,{href:"/documentation/pr-preview/pr-369/millicast/playback/players-sdks/",children:"Native SDK guides"})," in the Client SDKs section of the documentation."]}),"\n",(0,t.jsx)(n.h2,{id:"using-the-android-api",children:"Using the Android API"}),"\n",(0,t.jsx)(n.h3,{id:"creating-custom-audio-and-video-frame-classes-2",children:"Creating custom audio and video frame classes"}),"\n",(0,t.jsxs)(n.p,{children:["To ingest raw frames, you first must download and install the appropriate ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk/releases",children:"Millicast Native SDK"}),". Before you begin using the SDK, it is recommended you review the list of ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#millicast-native-sdk",children:"supported operating systems"}),", ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#audio-codecs-and-quality---encoding",children:"audio encoding limitations"}),", and ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk#video-codecs-and-quality---encoding",children:"video encoding limitations"})," as various limitations may impact your project. Additionally, if you encounter a bug, please ",(0,t.jsx)(n.a,{href:"https://github.com/millicast/millicast-native-sdk/issues",children:"open an issue"})," on the native project."]}),"\n",(0,t.jsxs)(n.p,{children:["Similar to the core API, we should begin by implementing the ",(0,t.jsx)(n.code,{children:"VideoFrame"})," interface and create a wrapper class for ",(0,t.jsx)(n.code,{children:"AudioFrame"}),":"]}),"\n",(0,t.jsx)(n.admonition,{title:"The Java API uses short to store the audio data.",type:"caution",children:(0,t.jsx)(n.p,{children:"In generating the sine values, which range between [0-1], we are required to scale the value to a larger range. Therefore, we use an amplitude of 2^15, which is essentially the size of a signed short."})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",children:'import com.millicast.VideoFrame;\nimport com.millicast.VideoType;\nimport com.millicast.AudioFrame;\n\nclass CustomVideoFrame implements VideoFrame {\n\n	private static final int WIDTH = 640;\n	private static final int HEIGHT = 480;\n\n    private byte _r, _g, _b;\n\n    public CustomVideoFrame(byte r, byte g, byte b) {\n        this._r = r;\n        this._g = g;\n        this._b = b;\n    }\n\n    @Override\n    public int width() {\n        return WIDTH;\n    }\n\n    @Override\n    public int height() {\n        return HEIGHT;\n    }\n\n	/**\n		Override this to return the corresponding frame type to the frame buffer you store\n    within the frame buffer. Video types supported can be found here\n		https://millicast.github.io/doc/latest/android/com/millicast/VideoType.html\n    */\n    @Override\n    public VideoType getType() {\n        return VideoType.ARGB;\n    }\n\n	/**\n		Override this to return the size corresponding to the frame type you\n    store within the frame buffer. For example, for I420 frames, the formula for calculating\n		the size is width * height * 3/2\n  	*/\n	@Override\n    public int size(VideoType videoType) {\n        return width() * height() * 4;\n    }\n\n	/**\n    Buffer is pre-allocated based on size. You are required to fill\n		the provided buffer with the frames you have, which should also correspond\n    to the getType() you defined earlier. For example, if you would like to\n    pass I420 frames, then getType() should return VideoType.I420\n		and your buffer should be of that type, and you should copy it into "buffer".\n  	*/\n    @Override\n    public void getBuffer(VideoType videoType, byte[] buffer) {\n        for(int i = 0; i < size(videoType); i+=4) {\n            buffer[i] = (byte) 255;\n            buffer[i+1] = _r;\n            buffer[i+2] = _g;\n            buffer[i+3] = _b;\n        }\n    }\n}\n\nclass CustomSineAudioFrame {\n    private final byte[] _audioData;\n\n    private int _sampleRate = 0;\n    private int _numChannels = 0;\n    private int _chunkTime = 0;\n\n    private int _sini = 0;\n\n    static final int TONE_FREQUENCY = 480; // 480Hz\n    static final int AMPLITUDE = (int) Math.pow(2, 15); // We will be amplifying the SINE since we are storing in\n    // short instead of float.\n    public CustomSineAudioFrame(int sampleRate, int numChannels, int chunkTime) {\n        _sampleRate = sampleRate;\n        _numChannels = numChannels;\n        _chunkTime = chunkTime;\n\n        int bufferSize = 2 * numChannels * sampleRate * chunkTime / 1000;\n\n\n        _audioData = new byte[bufferSize];\n    }\n    AudioFrame generate_sine_audio_frame() {\n        AudioFrame frame = new AudioFrame();\n        frame.bitsPerSample = 16;\n        frame.numChannels = _numChannels;\n        frame.sampleRate = _sampleRate;\n        frame.numFrames = _sampleRate * _chunkTime / 1000;\n\n        // our cycle\n\n        for(int j = 0; j < _audioData.length / 2; j+=_numChannels * 2) {\n            short value = (short) (AMPLITUDE * (Math.sin(_sini * TONE_FREQUENCY * 2.f * Math.PI / _sampleRate)));\n            byte[] valueAsBytes = ByteBuffer.allocate(2).putShort(value).order(ByteOrder.LITTLE_ENDIAN).array();\n            for(int k = 0; k < _numChannels; k++){\n                _audioData[j+2*k]   = valueAsBytes[0];\n                _audioData[j+2*k+1] = valueAsBytes[1];\n            }\n            _sini = (_sini + 1) % (_sampleRate / TONE_FREQUENCY);\n        }\n\n        frame.audioData = _audioData;\n        return frame;\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"creating-and-publishing-a-custom-source-2",children:"Creating and publishing a custom source"}),"\n",(0,t.jsx)(n.p,{children:"Similarly to the Core API example, do the following in order to create and publish a custom stream:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",children:"//... Include the frame classes here...\nimport com.millicast.CustomSource;\n\n/**\n  We will create timer tasks to perform our audio and video input. Assuming you\n  insert this in your class somewhere.\n*/\nprivate final Timer videoTimer = new Timer();\nprivate final Timer audioTimer = new Timer();\n\nprivate final TimerTask videoTask = new TimerTask() {\n    // Red frame\n    CustomVideoFrame frame = new CustomVideoFrame((byte) 255, (byte) 0, (byte) 0);\n\n    @Override\n    public void run() {\n        customSource.onVideoFrame(frame);\n    }\n};\n\nprivate final TimerTask audioTask = new TimerTask() {\n    final int SAMPLE_RATE = 48000; // 48kHz\n    final int NUM_CHANNELS = 2; // Stereo\n    final int CHUNK_TIME = 10; // Each sample equates to 10ms of playtime.\n\n    CustomSineAudioFrame frame_generator = new CustomSineAudioFrame(\n            SAMPLE_RATE, NUM_CHANNELS, CHUNK_TIME);\n    @Override\n    public void run() {\n        AudioFrame audio_frame = frame_generator.generate_sine_audio_frame();\n        customSource.onAudioFrame(audio_frame);\n    }\n};\n\n/**\n  Some method in your class\n*/\nvoid create_custom_source_and_publish() {\n  CustomSource source = CustomSource.create();\n\n  // Those tracks can now be added just as any other track to our publisher.\n  // (Refer to the other documentation to learn how to perform a simple publishing scenario)\n  VideoTrack videoTrack = (VideoTrack) source.startCaptureVideo();\n  AudioTrack audioTrack = (AudioTrack) source.startCaptureAudio();\n\n\n  // Start our timers\n  final int CHUNK_TIME = 10; // Each audio chunk is 10ms\n  final int FPS = 60; // We will be inputting frames at 60 frames per second.\n  videoTimer.schedule(videoTask, 0, 1000/FPS);\n  audioTimer.schedule(audioTask, 0, CHUNK_TIME);\n\n  // Connect and publish here ...\n\n  // When done, stop the capture\n  source.stopCapture();\n\n  // Make sure to join the threads here\n  videoThread.join();\n  audioThread.join();\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:["With the audio and video source instantiated, the frames can now be passed as inputs to the ",(0,t.jsx)(n.code,{children:"Publisher"})," object. To learn more about publishing a stream from the native SDKs explore the ",(0,t.jsx)(n.a,{href:"/documentation/pr-preview/pr-369/millicast/playback/players-sdks/",children:"Native SDK guides"})," in the Client SDKs section of the documentation."]})]})}function c(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},250065:function(e,n,i){i.d(n,{Z:()=>o,a:()=>s});var a=i(667294);let t={},r=a.createContext(t);function s(e){let n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(r.Provider,{value:n},e.children)}},982849:function(e){e.exports=JSON.parse('{"id":"broadcast/using-native-sdk-ingest-raw-frames","title":"Ingest Raw Frames","description":"Ingesting raw video and audio frames can be useful for creating and managing unique WebRTC encoding workflows. Whilst traditional encoders provide general-purpose encoding solutions, there are some use cases such as real-time streaming from drones where traditional encoding solutions are too heavy, power consumptive, or expensive to suffice.","source":"@site/millicast/broadcast/using-native-sdk-ingest-raw-frames.md","sourceDirName":"broadcast","slug":"/broadcast/using-native-sdk-ingest-raw-frames","permalink":"/documentation/pr-preview/pr-369/millicast/broadcast/using-native-sdk-ingest-raw-frames","draft":false,"unlisted":false,"editUrl":"https://github.com/THEOplayer/documentation/blob/-/millicast/broadcast/using-native-sdk-ingest-raw-frames.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Ingest Raw Frames","sidebar_position":11},"sidebar":"millicast","previous":{"title":"High Availability Management","permalink":"/documentation/pr-preview/pr-369/millicast/broadcast/redundant-ingest/high-availability-management"},"next":{"title":"Hardware Encoders","permalink":"/documentation/pr-preview/pr-369/millicast/hardware-encoders"}}')}}]);